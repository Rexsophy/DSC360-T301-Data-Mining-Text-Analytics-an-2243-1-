{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e3f6948",
   "metadata": {},
   "source": [
    "#Week 4 Exercise 4.2 Author: Rex Gayas Course & Section: DSC360-T301 Data Mining: Text Analytics an (2243-1) Date: 07JAN2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91cc26c",
   "metadata": {},
   "source": [
    "In the text, there’s a text normalizer created – your assignment is to re-create that normalizer as a Python class that can be re-used (within a .py file). However, unlike the book author’s version, pass a Pandas Series (e.g., dataframe[‘column’]) to your normalize_corpus function and use apply/lambda for each cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdea4a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Define a class TextNormalizer\n",
    "class TextNormalizer:\n",
    "    def __init__(self):\n",
    "        # Define a customized set of stopwords as an example\n",
    "        self.stop_words = set(['a', 'an', 'the', 'and', 'or', 'for', 'to', 'of', 'in', 'on', 'at', 'by', 'up', 'out', 'as'])\n",
    "\n",
    "    def strip_html_tags(self, text):\n",
    "        # Define a function to strip HTML tags\n",
    "        pattern = re.compile('<.*?>')\n",
    "        return re.sub(pattern, '', text)\n",
    "\n",
    "    def remove_accented_chars(self, text):\n",
    "        # Define a function to remove accented characters\n",
    "        return text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "    def text_to_lower(self, text):\n",
    "        # Define a function to convert text to lowercase\n",
    "        return text.lower()\n",
    "\n",
    "    def remove_extra_newlines(self, text):\n",
    "        # Define a function to remove extra newlines\n",
    "        return re.sub(r'\\r\\n|\\r|\\n', ' ', text)\n",
    "\n",
    "    def remove_special_characters_and_digits(self, text, remove_digits=True):\n",
    "        # Define a function to remove special characters and digits\n",
    "        pattern = r'[^a-zA-Z\\s]' if not remove_digits else r'[^a-zA-Z0-9\\s]'\n",
    "        return re.sub(pattern, '', text)\n",
    "\n",
    "    def remove_extra_whitespace(self, text):\n",
    "        # Define a function to remove extra whitespaces\n",
    "        return re.sub(' +', ' ', text)\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        # Define a function to remove stopwords\n",
    "        return ' '.join([word for word in text.split() if word not in self.stop_words])\n",
    "\n",
    "    def normalize_corpus(self, corpus):\n",
    "        # Apply the preprocessing methods to the dataframe series using apply and lambda for each cleaning function\n",
    "        corpus = corpus.apply(lambda x: self.strip_html_tags(x))\n",
    "        corpus = corpus.apply(lambda x: self.remove_accented_chars(x))\n",
    "        corpus = corpus.apply(lambda x: self.text_to_lower(x))\n",
    "        corpus = corpus.apply(lambda x: self.remove_extra_newlines(x))\n",
    "        corpus = corpus.apply(lambda x: self.remove_special_characters_and_digits(x))\n",
    "        corpus = corpus.apply(lambda x: self.remove_extra_whitespace(x))\n",
    "        corpus = corpus.apply(lambda x: self.remove_stopwords(x))\n",
    "        return corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2db392",
   "metadata": {},
   "source": [
    "Using your new text normalizer, create a Jupyter Notebook that uses this class to clean up the text found in the file big.txt (that text file is in the GitHub for Week 4 repository). Your resulting text should be a (long) single stream of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db21c6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project gutenberg ebook adventures sherlock holmes sir arthur conan doyle 15 our series sir arthur conan doyle  copyright laws are changing all over world be sure check copyright laws your country before downloading redistributing this any other project gutenberg ebook  this header should be first thing seen when viewing this project gutenberg file please do not remove it do not change edit header without written permission  please read legal small print other information about ebook project gutenberg bottom this file included is important information about your specific rights restrictions how file may be used you can also find about how make donation project gutenberg how get involved   welcome world free plain vanilla electronic texts  ebooks readable both humans computers since 1971  these ebooks were prepared thousands volunteers   title adventures sherlock holmes  author sir arthur conan doyle  release date march 1999 ebook 1661 most recently updated november 29 2002  edition 12  language english  character set encoding ascii  start project gutenberg ebook adventures sherlock holmes     additional editing jose menendez    adventures sherlock holmes    sir arthur conan doyle  contents  i scandal bohemia ii redheaded league iii case identity iv boscombe valley mystery v five orange pips vi man with twisted lip vii adventure blue carbuncle viii adventure speckled band ix adventure engineers thumb x adventure noble bachelor xi adventure beryl coronet xii adventure copper beeches   adventure i scandal bohemia  i   sherlock holmes she is always woman i have seldom heard him mention her under any other name his eyes she eclipses predominates whole her sex it was not that he felt any emotion akin love irene adler all emotions that one particularly were abhorrent his cold precise but admirably balanced mind he was i take it most perfect reasoning observing machine that world has seen but lover he would have placed himself false position he never spoke softer passions save with gibe sneer they were admirable things observerexcellent drawing veil from mens motives actions but trained reasoner admit such intrusions into his own delicate finely adjusted temperament was introduce distracting factor which might throw doubt upon all his mental results grit sensitive instrument crack one his own highpower lenses would not be more disturbing than strong emotion nature such his yet there was but one woman him that woman was late irene adler dubious questionable memory  i had seen little holmes lately my marriage had drifted us away from each other my own complete happiness homecentred interests which rise around man who first finds himself master his own establishment were sufficient absorb all my attention while holmes who loathed every form society with his whole bohemian soul remained our lodgings baker street buried among his old books alternating from week week between cocaine ambition drowsiness drug fierce energy his own keen nature he was still ever deeply attracted study crime occupied his immense faculties extraordinary powers observation following those clues clearing those mysteries which had been abandoned hopeless official police from time time i heard some vague account his doings his summons odessa case trepoff murder his clearing singular tragedy atkinson brothers trincomalee finally mission which he had accomplished so delicately successfully reigning family holland beyond these signs his activity however which i merely shared with all readers daily press i knew little my former friend companion  one nightit was twentieth march 1888i was returning from journey patient i had now returned civil practice when my way led me through baker street i passed wellremembered door which must always be associated my mind with my wooing with dark incidents study scarlet i was seized with keen desire see holmes again know how he was employing his extraordinary powers his rooms were brilliantly lit even i looked i saw his tall spare figure pass twice dark silhouette against blind he was pacing room swiftly eagerly with his head sunk upon his chest his hands clasped behind him me who knew his every mood habit his attitude manner told their own story he was work again he had risen his drugcreated dreams was hot upon scent some new problem i rang bell was shown chamber which had formerly been part my own  his manner was not effusive it seldom was but he was glad i think see me with hardly word spoken but with kindly eye he waved me armchair threw across his case cigars indicated spirit case gasogene corner then he stood before fire looked me over his singular introspective fashion  wedlock suits you he remarked i think watson that you have put seven half pounds since i saw you  seven i answered  indeed i should have thought little more just trifle more i fancy watson practice again i observe you did not tell me that you intended go into harness  then how do you know  i see it i deduce it how do i know that you have been \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# GitHub link to the big.txt file\n",
    "file_url = 'https://raw.githubusercontent.com/bellevue-university/dsc360/main/12%20Week/week_4/big.txt'\n",
    "\n",
    "# Use requests to get the content of the file\n",
    "response = requests.get(file_url)\n",
    "\n",
    "# Check the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Use StringIO to simulate a file-like object\n",
    "    file_content = StringIO(response.text)\n",
    "    \n",
    "    # Read the content line by line\n",
    "    lines = file_content.readlines()\n",
    "    \n",
    "    # Create a DataFrame from the lines\n",
    "    df = pd.DataFrame(lines, columns=['text'])\n",
    "    \n",
    "    # Instantiate the TextNormalizer\n",
    "    normalizer = TextNormalizer()\n",
    "    \n",
    "    # Normalize the corpus\n",
    "    df['normalized'] = normalizer.normalize_corpus(df['text'])\n",
    "    \n",
    "    # Combine into a single stream of text\n",
    "    single_stream_text = df['normalized'].str.cat(sep=' ')\n",
    "    \n",
    "    # Print the first 5000 characters of the normalized text to check the output\n",
    "    print(single_stream_text[:5000])\n",
    "    \n",
    "    # Print a snippet to confirm\n",
    "else:\n",
    "    print(f\"Failed to download the file: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2037ad11",
   "metadata": {},
   "source": [
    "Using spaCy and NLTK, show the tokens, lemmas, parts of speech, and dependencies in the first 1,021 characters of big.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47ab10c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: The, Lemma: the, POS: DET, Dependency: det\n",
      "Token: Project, Lemma: Project, POS: PROPN, Dependency: compound\n",
      "Token: Gutenberg, Lemma: Gutenberg, POS: PROPN, Dependency: compound\n",
      "Token: EBook, Lemma: EBook, POS: PROPN, Dependency: nmod\n",
      "Token: of, Lemma: of, POS: ADP, Dependency: prep\n",
      "Token: The, Lemma: the, POS: DET, Dependency: det\n",
      "Token: Adventures, Lemma: Adventures, POS: PROPN, Dependency: pobj\n",
      "Token: of, Lemma: of, POS: ADP, Dependency: prep\n",
      "Token: Sherlock, Lemma: Sherlock, POS: PROPN, Dependency: compound\n",
      "Token: Holmes, Lemma: Holmes, POS: PROPN, Dependency: pobj\n",
      "Token: \n",
      ", Lemma: \n",
      ", POS: SPACE, Dependency: dep\n",
      "Token: by, Lemma: by, POS: ADP, Dependency: prep\n",
      "Token: Sir, Lemma: Sir, POS: PROPN, Dependency: compound\n",
      "Token: Arthur, Lemma: Arthur, POS: PROPN, Dependency: compound\n",
      "Token: Conan, Lemma: Conan, POS: PROPN, Dependency: compound\n",
      "Token: Doyle, Lemma: Doyle, POS: PROPN, Dependency: pobj\n",
      "Token: \n",
      ", Lemma: \n",
      ", POS: SPACE, Dependency: dep\n",
      "Token: (, Lemma: (, POS: PUNCT, Dependency: punct\n",
      "Token: #, Lemma: #, POS: SYM, Dependency: nmod\n",
      "Token: 15, Lemma: 15, POS: NUM, Dependency: appos\n",
      "Token: in, Lemma: in, POS: ADP, Dependency: prep\n",
      "Token: our, Lemma: our, POS: PRON, Dependency: poss\n",
      "Token: series, Lemma: series, POS: NOUN, Dependency: pobj\n",
      "Token: by, Lemma: by, POS: ADP, Dependency: agent\n",
      "Token: Sir, Lemma: Sir, POS: PROPN, Dependency: compound\n",
      "Token: Arthur, Lemma: Arthur, POS: PROPN, Dependency: compound\n",
      "Token: Conan, Lemma: Conan, POS: PROPN, Dependency: compound\n",
      "Token: Doyle, Lemma: Doyle, POS: PROPN, Dependency: pobj\n",
      "Token: ), Lemma: ), POS: PUNCT, Dependency: punct\n",
      "Token: \n",
      "\n",
      ", Lemma: \n",
      "\n",
      ", POS: SPACE, Dependency: dep\n",
      "Token: Copyright, Lemma: Copyright, POS: PROPN, Dependency: compound\n",
      "Token: laws, Lemma: law, POS: NOUN, Dependency: nsubj\n",
      "Token: are, Lemma: be, POS: AUX, Dependency: aux\n",
      "Token: changing, Lemma: change, POS: VERB, Dependency: ROOT\n",
      "Token: all, Lemma: all, POS: ADV, Dependency: advmod\n",
      "Token: over, Lemma: over, POS: ADP, Dependency: prep\n",
      "Token: the, Lemma: the, POS: DET, Dependency: det\n",
      "Token: world, Lemma: world, POS: NOUN, Dependency: pobj\n",
      "Token: ., Lemma: ., POS: PUNCT, Dependency: punct\n",
      "Token: Be, Lemma: be, POS: AUX, Dependency: ROOT\n",
      "Token: sure, Lemma: sure, POS: ADJ, Dependency: acomp\n",
      "Token: to, Lemma: to, POS: PART, Dependency: aux\n",
      "Token: check, Lemma: check, POS: VERB, Dependency: xcomp\n",
      "Token: the, Lemma: the, POS: DET, Dependency: det\n",
      "Token: \n",
      ", Lemma: \n",
      ", POS: SPACE, Dependency: dep\n",
      "Token: copyright, Lemma: copyright, POS: NOUN, Dependency: compound\n",
      "Token: laws, Lemma: law, POS: NOUN, Dependency: dobj\n",
      "Token: for, Lemma: for, POS: ADP, Dependency: prep\n",
      "Token: your, Lemma: your, POS: PRON, Dependency: poss\n",
      "Token: country, Lemma: country, POS: NOUN, Dependency: pobj\n",
      "Token: before, Lemma: before, POS: ADP, Dependency: prep\n",
      "Token: downloading, Lemma: download, POS: VERB, Dependency: pcomp\n",
      "Token: or, Lemma: or, POS: CCONJ, Dependency: cc\n",
      "Token: redistributing, Lemma: redistribute, POS: VERB, Dependency: conj\n",
      "Token: \n",
      ", Lemma: \n",
      ", POS: SPACE, Dependency: dep\n",
      "Token: this, Lemma: this, POS: PRON, Dependency: dobj\n",
      "Token: or, Lemma: or, POS: CCONJ, Dependency: cc\n",
      "Token: any, Lemma: any, POS: DET, Dependency: det\n",
      "Token: other, Lemma: other, POS: ADJ, Dependency: amod\n",
      "Token: Project, Lemma: Project, POS: PROPN, Dependency: compound\n",
      "Token: Gutenberg, Lemma: Gutenberg, POS: PROPN, Dependency: compound\n",
      "Token: eBook, Lemma: eBook, POS: PROPN, Dependency: conj\n",
      "Token: ., Lemma: ., POS: PUNCT, Dependency: punct\n",
      "Token: \n",
      "\n",
      ", Lemma: \n",
      "\n",
      ", POS: SPACE, Dependency: dep\n",
      "Token: This, Lemma: this, POS: DET, Dependency: det\n",
      "Token: header, Lemma: header, POS: NOUN, Dependency: nsubj\n",
      "Token: should, Lemma: should, POS: AUX, Dependency: aux\n",
      "Token: be, Lemma: be, POS: AUX, Dependency: ROOT\n",
      "Token: the, Lemma: the, POS: DET, Dependency: det\n",
      "Token: first, Lemma: first, POS: ADJ, Dependency: amod\n",
      "Token: thing, Lemma: thing, POS: NOUN, Dependency: attr\n",
      "Token: seen, Lemma: see, POS: VERB, Dependency: acl\n",
      "Token: when, Lemma: when, POS: SCONJ, Dependency: advmod\n",
      "Token: viewing, Lemma: view, POS: VERB, Dependency: advcl\n",
      "Token: this, Lemma: this, POS: DET, Dependency: det\n",
      "Token: Project, Lemma: Project, POS: PROPN, Dependency: compound\n",
      "Token: \n",
      ", Lemma: \n",
      ", POS: SPACE, Dependency: dep\n",
      "Token: Gutenberg, Lemma: Gutenberg, POS: PROPN, Dependency: compound\n",
      "Token: file, Lemma: file, POS: NOUN, Dependency: dobj\n",
      "Token: ., Lemma: ., POS: PUNCT, Dependency: punct\n",
      "Token:  , Lemma:  , POS: SPACE, Dependency: dep\n",
      "Token: Please, Lemma: please, POS: INTJ, Dependency: intj\n",
      "Token: do, Lemma: do, POS: AUX, Dependency: aux\n",
      "Token: not, Lemma: not, POS: PART, Dependency: neg\n",
      "Token: remove, Lemma: remove, POS: VERB, Dependency: ROOT\n",
      "Token: it, Lemma: it, POS: PRON, Dependency: dobj\n",
      "Token: ., Lemma: ., POS: PUNCT, Dependency: punct\n",
      "Token:  , Lemma:  , POS: SPACE, Dependency: dep\n",
      "Token: Do, Lemma: do, POS: AUX, Dependency: aux\n",
      "Token: not, Lemma: not, POS: PART, Dependency: neg\n",
      "Token: change, Lemma: change, POS: VERB, Dependency: ROOT\n",
      "Token: or, Lemma: or, POS: CCONJ, Dependency: cc\n",
      "Token: edit, Lemma: edit, POS: VERB, Dependency: conj\n",
      "Token: the, Lemma: the, POS: DET, Dependency: det\n",
      "Token: \n",
      ", Lemma: \n",
      ", POS: SPACE, Dependency: dep\n",
      "Token: header, Lemma: header, POS: NOUN, Dependency: dobj\n",
      "Token: without, Lemma: without, POS: ADP, Dependency: prep\n",
      "Token: written, Lemma: write, POS: VERB, Dependency: amod\n",
      "Token: permission, Lemma: permission, POS: NOUN, Dependency: pobj\n",
      "Token: ., Lemma: ., POS: PUNCT, Dependency: punct\n",
      "Token: \n",
      "\n",
      ", Lemma: \n",
      "\n",
      ", POS: SPACE, Dependency: dep\n",
      "Token: Please, Lemma: please, POS: INTJ, Dependency: intj\n",
      "Token: read, Lemma: read, POS: VERB, Dependency: ROOT\n",
      "Token: the, Lemma: the, POS: DET, Dependency: det\n",
      "Token: \", Lemma: \", POS: PUNCT, Dependency: punct\n",
      "Token: legal, Lemma: legal, POS: ADJ, Dependency: amod\n",
      "Token: small, Lemma: small, POS: ADJ, Dependency: amod\n",
      "Token: print, Lemma: print, POS: NOUN, Dependency: dobj\n",
      "Token: ,, Lemma: ,, POS: PUNCT, Dependency: punct\n",
      "Token: \", Lemma: \", POS: PUNCT, Dependency: punct\n",
      "Token: and, Lemma: and, POS: CCONJ, Dependency: cc\n",
      "Token: other, Lemma: other, POS: ADJ, Dependency: amod\n",
      "Token: information, Lemma: information, POS: NOUN, Dependency: conj\n",
      "Token: about, Lemma: about, POS: ADP, Dependency: prep\n",
      "Token: the, Lemma: the, POS: DET, Dependency: det\n",
      "Token: \n",
      ", Lemma: \n",
      ", POS: SPACE, Dependency: dep\n",
      "Token: eBook, Lemma: eBook, POS: PROPN, Dependency: nmod\n",
      "Token: and, Lemma: and, POS: CCONJ, Dependency: cc\n",
      "Token: Project, Lemma: Project, POS: PROPN, Dependency: conj\n",
      "Token: Gutenberg, Lemma: Gutenberg, POS: PROPN, Dependency: pobj\n",
      "Token: at, Lemma: at, POS: ADP, Dependency: prep\n",
      "Token: the, Lemma: the, POS: DET, Dependency: det\n",
      "Token: bottom, Lemma: bottom, POS: NOUN, Dependency: pobj\n",
      "Token: of, Lemma: of, POS: ADP, Dependency: prep\n",
      "Token: this, Lemma: this, POS: DET, Dependency: det\n",
      "Token: file, Lemma: file, POS: NOUN, Dependency: pobj\n",
      "Token: ., Lemma: ., POS: PUNCT, Dependency: punct\n",
      "Token:  , Lemma:  , POS: SPACE, Dependency: dep\n",
      "Token: Included, Lemma: include, POS: VERB, Dependency: csubj\n",
      "Token: is, Lemma: be, POS: AUX, Dependency: ROOT\n",
      "Token: \n",
      ", Lemma: \n",
      ", POS: SPACE, Dependency: dep\n",
      "Token: important, Lemma: important, POS: ADJ, Dependency: amod\n",
      "Token: information, Lemma: information, POS: NOUN, Dependency: attr\n",
      "Token: about, Lemma: about, POS: ADP, Dependency: prep\n",
      "Token: your, Lemma: your, POS: PRON, Dependency: poss\n",
      "Token: specific, Lemma: specific, POS: ADJ, Dependency: amod\n",
      "Token: rights, Lemma: right, POS: NOUN, Dependency: pobj\n",
      "Token: and, Lemma: and, POS: CCONJ, Dependency: cc\n",
      "Token: restrictions, Lemma: restriction, POS: NOUN, Dependency: conj\n",
      "Token: in, Lemma: in, POS: ADP, Dependency: prep\n",
      "Token: \n",
      ", Lemma: \n",
      ", POS: SPACE, Dependency: dep\n",
      "Token: how, Lemma: how, POS: SCONJ, Dependency: advmod\n",
      "Token: the, Lemma: the, POS: DET, Dependency: det\n",
      "Token: file, Lemma: file, POS: NOUN, Dependency: nsubjpass\n",
      "Token: may, Lemma: may, POS: AUX, Dependency: aux\n",
      "Token: be, Lemma: be, POS: AUX, Dependency: auxpass\n",
      "Token: used, Lemma: use, POS: VERB, Dependency: pcomp\n",
      "Token: ., Lemma: ., POS: PUNCT, Dependency: punct\n",
      "Token:  , Lemma:  , POS: SPACE, Dependency: dep\n",
      "Token: You, Lemma: you, POS: PRON, Dependency: nsubj\n",
      "Token: can, Lemma: can, POS: AUX, Dependency: aux\n",
      "Token: also, Lemma: also, POS: ADV, Dependency: advmod\n",
      "Token: find, Lemma: find, POS: VERB, Dependency: ROOT\n",
      "Token: out, Lemma: out, POS: ADP, Dependency: prt\n",
      "Token: about, Lemma: about, POS: ADP, Dependency: prep\n",
      "Token: how, Lemma: how, POS: SCONJ, Dependency: advmod\n",
      "Token: to, Lemma: to, POS: PART, Dependency: aux\n",
      "Token: make, Lemma: make, POS: VERB, Dependency: pcomp\n",
      "Token: a, Lemma: a, POS: DET, Dependency: det\n",
      "Token: \n",
      ", Lemma: \n",
      ", POS: SPACE, Dependency: dep\n",
      "Token: donation, Lemma: donation, POS: NOUN, Dependency: dobj\n",
      "Token: to, Lemma: to, POS: ADP, Dependency: prep\n",
      "Token: Project, Lemma: Project, POS: PROPN, Dependency: compound\n",
      "Token: Gutenberg, Lemma: Gutenberg, POS: PROPN, Dependency: pobj\n",
      "Token: ,, Lemma: ,, POS: PUNCT, Dependency: punct\n",
      "Token: and, Lemma: and, POS: CCONJ, Dependency: cc\n",
      "Token: how, Lemma: how, POS: SCONJ, Dependency: advmod\n",
      "Token: to, Lemma: to, POS: PART, Dependency: aux\n",
      "Token: get, Lemma: get, POS: AUX, Dependency: auxpass\n",
      "Token: involved, Lemma: involve, POS: VERB, Dependency: conj\n",
      "Token: ., Lemma: ., POS: PUNCT, Dependency: punct\n",
      "Token: \n",
      "\n",
      "\n",
      ", Lemma: \n",
      "\n",
      "\n",
      ", POS: SPACE, Dependency: dep\n",
      "Token: *, Lemma: *, POS: PUNCT, Dependency: punct\n",
      "Token: *, Lemma: *, POS: PUNCT, Dependency: dep\n",
      "Token: Welcome, Lemma: welcome, POS: ADJ, Dependency: appos\n",
      "Token: To, Lemma: to, POS: ADP, Dependency: prep\n",
      "Token: The, Lemma: the, POS: DET, Dependency: det\n",
      "Token: World, Lemma: World, POS: PROPN, Dependency: pobj\n",
      "Token: of, Lemma: of, POS: ADP, Dependency: prep\n",
      "Token: Free, Lemma: Free, POS: PROPN, Dependency: compound\n",
      "Token: Plain, Lemma: Plain, POS: PROPN, Dependency: compound\n",
      "Token: Vanilla, Lemma: Vanilla, POS: PROPN, Dependency: compound\n",
      "Token: Electronic, Lemma: Electronic, POS: PROPN, Dependency: compound\n",
      "Token: Texts, Lemma: Texts, POS: PROPN, Dependency: pobj\n",
      "Token: *, Lemma: *, POS: PUNCT, Dependency: punct\n",
      "Token: *, Lemma: *, POS: PUNCT, Dependency: punct\n",
      "Token: \n",
      "\n",
      ", Lemma: \n",
      "\n",
      ", POS: SPACE, Dependency: dep\n",
      "Token: *, Lemma: *, POS: PUNCT, Dependency: punct\n",
      "Token: *, Lemma: *, POS: PUNCT, Dependency: punct\n",
      "Token: eBooks, Lemma: eBooks, POS: PROPN, Dependency: compound\n",
      "Token: Readable, Lemma: readable, POS: ADJ, Dependency: advcl\n",
      "Token: By, Lemma: by, POS: ADP, Dependency: prep\n",
      "Token: Both, Lemma: both, POS: DET, Dependency: det\n",
      "Token: Humans, Lemma: Humans, POS: PROPN, Dependency: pobj\n",
      "Token: and, Lemma: and, POS: CCONJ, Dependency: cc\n",
      "Token: By, Lemma: by, POS: ADP, Dependency: conj\n",
      "Token: Computers, Lemma: computer, POS: NOUN, Dependency: pobj\n",
      "Token: ,, Lemma: ,, POS: PUNCT, Dependency: punct\n",
      "Token: Since, Lemma: since, POS: SCONJ, Dependency: mark\n",
      "Token: 1971, Lemma: 1971, POS: NUM, Dependency: pobj\n",
      "Token: *, Lemma: *, POS: PUNCT, Dependency: pobj\n",
      "Token: *, Lemma: *, POS: PUNCT, Dependency: pobj\n",
      "Token: \n",
      "\n",
      ", Lemma: \n",
      "\n",
      ", POS: SPACE, Dependency: dep\n",
      "Token: *, Lemma: *, POS: PUNCT, Dependency: pobj\n",
      "Token: *, Lemma: *, POS: PUNCT, Dependency: pobj\n",
      "Token: *, Lemma: *, POS: PUNCT, Dependency: nsubjpass\n",
      "Token: *, Lemma: *, POS: PUNCT, Dependency: nsubjpass\n",
      "Token: *, Lemma: *, POS: PUNCT, Dependency: nsubjpass\n",
      "Token: These, Lemma: These, POS: PROPN, Dependency: det\n",
      "Token: eBooks, Lemma: eBooks, POS: PROPN, Dependency: nsubjpass\n",
      "Token: Were, Lemma: be, POS: AUX, Dependency: auxpass\n",
      "Token: Prepared, Lemma: prepare, POS: VERB, Dependency: ROOT\n",
      "Token: By, Lemma: by, POS: ADP, Dependency: prep\n",
      "Token: Thousands, Lemma: thousand, POS: NOUN, Dependency: pobj\n",
      "Token: of, Lemma: of, POS: ADP, Dependency: prep\n",
      "Token: Volunteers, Lemma: Volunteers, POS: PROPN, Dependency: pobj\n",
      "Token: !, Lemma: !, POS: PUNCT, Dependency: punct\n",
      "Token: *, Lemma: *, POS: PUNCT, Dependency: ROOT\n",
      "Token: *, Lemma: *, POS: PUNCT, Dependency: ROOT\n",
      "Token: *, Lemma: *, POS: PUNCT, Dependency: punct\n",
      "Token: *, Lemma: *, POS: PUNCT, Dependency: conj\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import requests\n",
    "import nltk\n",
    "\n",
    "# Utilize spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# GitHub link to the big.txt file\n",
    "file_url = \"https://raw.githubusercontent.com/bellevue-university/dsc360/main/12%20Week/week_4/big.txt\"\n",
    "\n",
    "# Download the first part of the file\n",
    "response = requests.get(file_url)\n",
    "if response.status_code != 200:\n",
    "    raise Exception(\"Failed to download the file\")\n",
    "\n",
    "# Extract the first 1021 characters as per exercise 3's request\n",
    "text = response.text[:1021]\n",
    "\n",
    "# Process text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print tokens, lemmas, parts of speech, and dependencies as per exercise 3's request\n",
    "for token in doc:\n",
    "    print(f\"Token: {token.text}, Lemma: {token.lemma_}, POS: {token.pos_}, Dependency: {token.dep_}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d11366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
