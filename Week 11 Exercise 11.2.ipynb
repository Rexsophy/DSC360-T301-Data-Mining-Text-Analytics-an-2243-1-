{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6190d8c-eb71-418a-89d6-b32b2769bc25",
   "metadata": {},
   "source": [
    "#### Week 11 Exercise 11.2 Author: Rex Gayas Course & Section: DSC360-T301 Data Mining: Text Analytics an (2243-1) Date: 25 FEB 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcbbde1-b439-4d9e-9dbf-5c4e275b5113",
   "metadata": {},
   "source": [
    "#### Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "797eea7c-336d-4a17-82cb-53dea1147d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Description</th>\n",
       "      <th>Browser_Used</th>\n",
       "      <th>Device_Used</th>\n",
       "      <th>Is_Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id10326</td>\n",
       "      <td>The room was kind of clean but had a VERY stro...</td>\n",
       "      <td>Edge</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>not happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id10327</td>\n",
       "      <td>I stayed at the Crown Plaza April -- - April -...</td>\n",
       "      <td>Internet Explorer</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>not happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id10328</td>\n",
       "      <td>I booked this hotel through Hotwire at the low...</td>\n",
       "      <td>Mozilla</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>not happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id10329</td>\n",
       "      <td>Stayed here with husband and sons on the way t...</td>\n",
       "      <td>InternetExplorer</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id10330</td>\n",
       "      <td>My girlfriends and I stayed here to celebrate ...</td>\n",
       "      <td>Edge</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>not happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID                                        Description  \\\n",
       "0  id10326  The room was kind of clean but had a VERY stro...   \n",
       "1  id10327  I stayed at the Crown Plaza April -- - April -...   \n",
       "2  id10328  I booked this hotel through Hotwire at the low...   \n",
       "3  id10329  Stayed here with husband and sons on the way t...   \n",
       "4  id10330  My girlfriends and I stayed here to celebrate ...   \n",
       "\n",
       "        Browser_Used Device_Used Is_Response  \n",
       "0               Edge      Mobile   not happy  \n",
       "1  Internet Explorer      Mobile   not happy  \n",
       "2            Mozilla      Tablet   not happy  \n",
       "3   InternetExplorer     Desktop       happy  \n",
       "4               Edge      Tablet   not happy  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r'D:\\ALPHA\\Dynamic Folder\\Bellevue\\Winter 2023\\Data Mining\\Week 11\\archive\\hotel-reviews.csv'\n",
    "hotel_reviews = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few entries to understand the dataset structure\n",
    "hotel_reviews.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29ce15a-dd71-4c54-bfdf-5588921ed0cd",
   "metadata": {},
   "source": [
    "For building the deep learning model, the focus is in the Description column as input, and Is_Response as the target variable. First task is to encode the Is_Response to a binary variable, where “happy” can be 1, and “not happy” can be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cddeb8-5d63-4fb2-a344-54118f5ea1d8",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "337c1c04-24a0-4896-899e-a9771b5fbddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5031     we stayed in a suite and found it spacious eno...\n",
       " 28166    my boyfriend and i were given a day trip to ne...\n",
       " 11229    we stayed at the hotel for a week in sept over...\n",
       " 7346     i stayed here for my st birthday with other pe...\n",
       " 6316     the location of this hotel is great and thats ...\n",
       " Name: Cleaned_Description, dtype: object,\n",
       " 5031     1\n",
       " 28166    1\n",
       " 11229    1\n",
       " 7346     1\n",
       " 6316     0\n",
       " Name: Is_Response, dtype: int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Preprocessing text data\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Remove excess whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Clean the descriptions\n",
    "hotel_reviews['Cleaned_Description'] = hotel_reviews['Description'].apply(clean_text)\n",
    "\n",
    "# Encode the target variable\n",
    "hotel_reviews['Is_Response'] = hotel_reviews['Is_Response'].map({'happy': 1, 'not happy': 0})\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    hotel_reviews['Cleaned_Description'], \n",
    "    hotel_reviews['Is_Response'], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Check the results\n",
    "X_train.head(), y_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f49dda-a71e-4a8e-92db-ab7ece695757",
   "metadata": {},
   "source": [
    "A text cleaning function was defined and applied to the “Description” column to preprocess the data. The target variable “Is_Response” was encoded to binary format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a4c236-4b0f-4757-891a-f663dfe5cec5",
   "metadata": {},
   "source": [
    "#### Universal Sentence Encoder Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20c2202f-5a40-4e1b-8457-446cf04b33ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('D:\\\\ALPHA\\\\Dynamic Folder\\\\Bellevue\\\\Winter 2023\\\\Data Mining\\\\Week 11\\\\archive\\\\hotel-reviews.csv')\n",
    "\n",
    "# Clean function redefined\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning function to the 'Description' column\n",
    "df['Description'] = df['Description'].apply(clean_text)\n",
    "\n",
    "# Encode the target variable 'Is_Response'\n",
    "df['Is_Response'] = df['Is_Response'].map({'happy': 1, 'not happy': 0})\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Description'], df['Is_Response'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the Universal Sentence Encoder's TF Hub module\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "embed = hub.load(module_url)\n",
    "\n",
    "# Function to create sentence embeddings with batch processing\n",
    "def get_sentence_embeddings(sentences, batch_size=128):\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[i:i+batch_size]\n",
    "        batch_embeddings = embed(batch_sentences)\n",
    "        all_embeddings.append(batch_embeddings.numpy())\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# Convert sentences in training and testing sets into embeddings\n",
    "X_train_embeddings = get_sentence_embeddings(X_train.tolist())\n",
    "X_test_embeddings = get_sentence_embeddings(X_test.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ffe5148-2b3a-4cff-8b64-30c6b48a0754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31145, 512)\n",
      "(7787, 512)\n",
      "[[-0.02875476 -0.01960756  0.00453075 ...  0.04044104  0.04497837\n",
      "   0.05902657]\n",
      " [-0.03446922 -0.03135321 -0.01324397 ...  0.04881332 -0.00763081\n",
      "   0.05488839]]\n",
      "[[-0.03608064  0.02838305 -0.02954326 ...  0.01310635 -0.01404778\n",
      "   0.04856294]\n",
      " [-0.06717499 -0.04941348 -0.02446725 ... -0.01657945  0.04211202\n",
      "   0.0501585 ]]\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the embeddings to confirm their dimensions\n",
    "print(X_train_embeddings.shape)\n",
    "print(X_test_embeddings.shape)\n",
    "\n",
    "# Print a small part of the embeddings to see their actual values\n",
    "print(X_train_embeddings[:2])  # Prints the first two embeddings from the training set\n",
    "print(X_test_embeddings[:2])   # Prints the first two embeddings from the testing set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaf88d0-75b0-4d64-b5e3-42777d2a9f3b",
   "metadata": {},
   "source": [
    "Given the nature of the USE which is memory-intensive, the code was adjusted to process the sentence embeddings in batches to avoid overwhelming the system's memory. The output shows there are 31,145 training samples and 7,787 testing samples, each represented by a 512-dimensional embedding vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701f0c89-5feb-4c36-883e-a3409db6ddb9",
   "metadata": {},
   "source": [
    "#### Model Building, Training, and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "171a7aae-530e-4b7d-9c45-d54605c7536e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "876/876 [==============================] - 3s 3ms/step - loss: 0.3476 - accuracy: 0.8478 - val_loss: 0.2968 - val_accuracy: 0.8722\n",
      "Epoch 2/10\n",
      "876/876 [==============================] - 2s 2ms/step - loss: 0.3152 - accuracy: 0.8641 - val_loss: 0.2895 - val_accuracy: 0.8777\n",
      "Epoch 3/10\n",
      "876/876 [==============================] - 2s 2ms/step - loss: 0.3092 - accuracy: 0.8675 - val_loss: 0.2846 - val_accuracy: 0.8796\n",
      "Epoch 4/10\n",
      "876/876 [==============================] - 2s 2ms/step - loss: 0.3062 - accuracy: 0.8693 - val_loss: 0.2836 - val_accuracy: 0.8780\n",
      "Epoch 5/10\n",
      "876/876 [==============================] - 2s 2ms/step - loss: 0.3014 - accuracy: 0.8719 - val_loss: 0.2861 - val_accuracy: 0.8831\n",
      "Epoch 6/10\n",
      "876/876 [==============================] - 2s 2ms/step - loss: 0.2983 - accuracy: 0.8727 - val_loss: 0.2935 - val_accuracy: 0.8735\n",
      "Epoch 7/10\n",
      "876/876 [==============================] - 2s 2ms/step - loss: 0.2949 - accuracy: 0.8751 - val_loss: 0.2834 - val_accuracy: 0.8831\n",
      "Epoch 8/10\n",
      "876/876 [==============================] - 2s 2ms/step - loss: 0.2923 - accuracy: 0.8756 - val_loss: 0.2856 - val_accuracy: 0.8787\n",
      "Epoch 9/10\n",
      "876/876 [==============================] - 2s 2ms/step - loss: 0.2867 - accuracy: 0.8772 - val_loss: 0.2869 - val_accuracy: 0.8809\n",
      "Epoch 10/10\n",
      "876/876 [==============================] - 2s 2ms/step - loss: 0.2825 - accuracy: 0.8800 - val_loss: 0.2830 - val_accuracy: 0.8838\n",
      "244/244 [==============================] - 0s 1ms/step - loss: 0.3053 - accuracy: 0.8673\n",
      "Test Accuracy: 0.867343008518219\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(512,)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Using sigmoid for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_embeddings, y_train, epochs=10, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_embeddings, y_test)\n",
    "print('Test Accuracy:', test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650ad72c-3a6a-4ee5-a16e-4d6bb30453ba",
   "metadata": {},
   "source": [
    "The trained model was evaluated on the test set, yielding an accuracy of approximately 86.90%. Throughout the process, memory limitations were carefully considered, especially when generating embeddings for the entire dataset. Due to kernel crashes, batch processing was employed to mitigate potential memory issues. For consideration, the model can be fine-tuned by adjusting hyperparameters. K-fold cross-validation is also helpful in assessing consistency across different subsets. Error analysis can also be implemented to understand the origin of mistakes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
