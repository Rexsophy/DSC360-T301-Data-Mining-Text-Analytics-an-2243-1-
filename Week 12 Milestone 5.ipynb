{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "829fac6b-113d-4746-8600-1dd549a25726",
   "metadata": {},
   "source": [
    "#### Week 12 Milestone 5 Author: Rex Gayas Course & Section: DSC360-T301 Data Mining: Text Analytics an (2243-1) Date: 02 MAR 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c86498-339c-4328-9c63-d565795b28a6",
   "metadata": {},
   "source": [
    "#### Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c180ccef-8e62-40cb-b631-c749d88f1d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment                                            message  \\\n",
      "0         -1  @tiniebeany climate change is an interesting h...   \n",
      "1          1  RT @NatGeoChannel: Watch #BeforeTheFlood right...   \n",
      "2          1  Fabulous! Leonardo #DiCaprio's film on #climat...   \n",
      "3          1  RT @Mick_Fanning: Just watched this amazing do...   \n",
      "4          2  RT @cnalive: Pranita Biswasi, a Lutheran from ...   \n",
      "\n",
      "              tweetid                                       cleaned_text  \n",
      "0  792927353886371840  climate change interesting hustle global warmi...  \n",
      "1  793124211518832641  rt : watch beforetheflood right , travels worl...  \n",
      "2  793124402388832256  fabulous ! leonardo dicaprio 's film climate c...  \n",
      "3  793124635873275904  rt : watched amazing documentary leonardodicap...  \n",
      "4  793125156185137153  rt : pranita biswasi , lutheran odisha , gives...  \n",
      "       comment_id  score                                          self_text  \\\n",
      "268628    k14k1rf     16  They are taking the moral stance without putti...   \n",
      "273207    k0b6d9w      6  ...these ambulance chasers, if there's mysery ...   \n",
      "75088     khqn0hw      2  Honestly, still surprises me! Easy mistake to ...   \n",
      "275502    jzuxgzt      5  Does your timeline take into account feedback ...   \n",
      "146191    kcvb571      4             With a cow doing methane measurements.   \n",
      "\n",
      "              subreddit         created_time  post_id           author_name  \\\n",
      "268628  climateskeptics  2023-09-18 13:56:28  16lqdo9           KELEVRACMDR   \n",
      "273207  climateskeptics  2023-09-12 20:37:42  16gzitu  Illustrious_Pepper46   \n",
      "75088     unitedkingdom  2024-01-13 23:41:25  195madt        oxygenthievery   \n",
      "275502    climatechange  2023-09-09 19:58:49  16e8suv           Villager723   \n",
      "146191           canada  2023-12-11 05:39:55  18fkuh3      Distinct-Lynx300   \n",
      "\n",
      "        controversiality  ups  downs  ...  user_comment_karma  \\\n",
      "268628                 0   16      0  ...              3547.0   \n",
      "273207                 0    6      0  ...             12485.0   \n",
      "75088                  0    2      0  ...              2623.0   \n",
      "275502                 0    5      0  ...             69074.0   \n",
      "146191                 0    4      0  ...              4120.0   \n",
      "\n",
      "       user_total_karma  post_score  \\\n",
      "268628           4058.0         174   \n",
      "273207          14968.0          13   \n",
      "75088            2735.0         147   \n",
      "275502          71197.0          25   \n",
      "146191           7256.0          22   \n",
      "\n",
      "                                           post_self_text  \\\n",
      "268628                                                NaN   \n",
      "273207                                                NaN   \n",
      "75088                                                 NaN   \n",
      "275502  Hi,\\n\\nI hate summer, I hate heat, I hate suns...   \n",
      "146191                                                NaN   \n",
      "\n",
      "                                               post_title  post_upvote_ratio  \\\n",
      "268628            She drank the \"Climate Crisis\" Kool-Aid               0.88   \n",
      "273207  Libya floods wipe out quarter of city, death t...               0.79   \n",
      "75088   More than 1km under the North Sea is a climate...               0.92   \n",
      "275502               Where will it still be wet and cool?               0.86   \n",
      "146191  Canada Tackles Methane Emissions From Cow Burp...               0.58   \n",
      "\n",
      "        post_thumbs_ups  post_total_awards_received    post_created_time  \\\n",
      "268628              174                           0  2023-09-18 09:05:20   \n",
      "273207               13                           0  2023-09-12 18:49:19   \n",
      "75088               147                           0  2024-01-13 11:47:21   \n",
      "275502               25                           0  2023-09-09 15:39:11   \n",
      "146191               22                           0  2023-12-11 03:10:17   \n",
      "\n",
      "                                             cleaned_text  \n",
      "268628  taking moral stance without putting work . hit...  \n",
      "273207  ... ambulance chasers , 's mysery world , find...  \n",
      "75088   honestly , still surprises ! easy mistake make...  \n",
      "275502  timeline take account feedback loops , methane...  \n",
      "146191                         cow methane measurements .  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Define a function to clean the text\n",
    "def clean_text(text):\n",
    "    # If text is NaN (float), return an empty string\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs, hashtags, mentions, and stopwords\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # Join the tokens back into a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "# Load Twitter dataset\n",
    "twitter_file_path = 'D:/ALPHA\\Dynamic Folder/Bellevue/Winter 2023/Data Mining/Project/Datasets/Kaggle/twitter_sentiment_data.csv'  \n",
    "twitter_df = pd.read_csv(twitter_file_path)\n",
    "twitter_df['cleaned_text'] = twitter_df['message'].apply(clean_text)\n",
    "\n",
    "# Load Reddit dataset and sample 10% of the data\n",
    "reddit_file_path = 'D:/ALPHA/Dynamic Folder/Bellevue/Winter 2023/Data Mining/Project/Datasets/Kaggle/reddit_opinion_climate_change.csv' \n",
    "reddit_df = pd.read_csv(reddit_file_path)\n",
    "reddit_sample_df = reddit_df.sample(frac=0.1, random_state=42)  # Sample 10% of the data\n",
    "reddit_sample_df['cleaned_text'] = reddit_sample_df['self_text'].apply(clean_text)\n",
    "\n",
    "# Display the first few rows to confirm successful loading and cleaning\n",
    "print(twitter_df.head())\n",
    "print(reddit_sample_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c0aa4b-1dce-41bf-90e2-d8cbcf2567f3",
   "metadata": {},
   "source": [
    "Implemented data loading and preprocessing steps for sentiment analysis. Imported necessary libraries for data manipulation, text feature extraction, and natural language processing. Defined a “clean_text” function that normalizes the text by converting it to lowercase, removing URLs, hashtags, mentions, and stopwords, and tokenizing the text. Function is applied to the “message” column of the Twitter dataset and the “self_text” column of a sample of the Reddit dataset to clean and prepare the text data for vectorization and model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b0a60ed-dd93-4f97-8021-9122e5b6c49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     source                            author  \\\n",
      "0    {'id': 'bbc-news', 'name': 'BBC News'}  https://www.facebook.com/bbcnews   \n",
      "1  {'id': 'the-verge', 'name': 'The Verge'}                   Jess Weatherbed   \n",
      "2  {'id': 'the-verge', 'name': 'The Verge'}                     Justine Calma   \n",
      "3  {'id': 'the-verge', 'name': 'The Verge'}                     Justine Calma   \n",
      "4          {'id': 'wired', 'name': 'Wired'}                 Stephen Armstrong   \n",
      "\n",
      "                                               title  \\\n",
      "0  Climate change may explain rare dragonfly's sp...   \n",
      "1                  Paris votes to crack down on SUVs   \n",
      "2  The threat of extinction is getting worse for ...   \n",
      "3  NASA’s new mission will study microscopic plan...   \n",
      "4  Climate Finance Is Targeting the Wrong Industries   \n",
      "\n",
      "                                         description  \\\n",
      "0  Experts say rising temperatures may have encou...   \n",
      "1  Parisians have voted to increase parking charg...   \n",
      "2  A new United Nations report gives the fullest ...   \n",
      "3  A new NASA mission will study microscopic plan...   \n",
      "4  Roughly half of the world’s emissions currentl...   \n",
      "\n",
      "                                                 url  \\\n",
      "0     https://www.bbc.co.uk/news/uk-england-68406076   \n",
      "1  https://www.theverge.com/2024/2/5/24061970/suv...   \n",
      "2  https://www.theverge.com/2024/2/13/24071713/mi...   \n",
      "3  https://www.theverge.com/2024/2/8/24066014/nas...   \n",
      "4  https://www.wired.com/story/climate-finance-wr...   \n",
      "\n",
      "                                          urlToImage           publishedAt  \\\n",
      "0  https://ichef.bbci.co.uk/news/1024/branded_new...  2024-02-27T06:34:39Z   \n",
      "1  https://cdn.vox-cdn.com/thumbor/OstemqLr6gLbCl...  2024-02-05T12:24:14Z   \n",
      "2  https://cdn.vox-cdn.com/thumbor/h5kNb06_KICKtr...  2024-02-13T20:00:51Z   \n",
      "3  https://cdn.vox-cdn.com/thumbor/oQ-lJOR2I3m-NL...  2024-02-08T19:12:39Z   \n",
      "4  https://media.wired.com/photos/65caaab26ec7723...  2024-02-13T13:50:49Z   \n",
      "\n",
      "                                             content  \n",
      "0  A once-rare type of dragonfly may have been ai...  \n",
      "1  Paris votes to crack down on SUVs\\r\\nParis vot...  \n",
      "2  The endangered Steppe Eagle is one of the migr...  \n",
      "3  NASAs new mission will study microscopic plank...  \n",
      "4  To achieve net-zero carbon emissions by 2030, ...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# NewsAPI key\n",
    "api_key = '586380ef164f4b9287f2e14a286cc9ef'\n",
    "\n",
    "# Define the endpoint and parameters\n",
    "url = 'https://newsapi.org/v2/everything'\n",
    "params = {\n",
    "    'q': 'climate change',  # Query for articles mentioning climate change\n",
    "    'apiKey': api_key,\n",
    "    'pageSize': 20,  # Number of articles to return\n",
    "    'page': 1  # Page number \n",
    "}\n",
    "\n",
    "# Make the request to the NewsAPI\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    articles = response.json()['articles']\n",
    "    # Create a DataFrame with the relevant information\n",
    "    df_articles = pd.DataFrame(articles)\n",
    "    print(df_articles.head())  # Display the first few entries\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfc187f-f6a8-4a99-b42d-0c48d3ad933e",
   "metadata": {},
   "source": [
    "Fetched articles related to climate change from the News API. Set up the necessary parameters, including the API key, the query term (“climate change”), and the desired number of articles to retrieve. The  “requests” library is used to make a GET request to the News API with these parameters, and the response is checked for success (HTTP status code 200). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89abe746-6aa0-4464-a59d-ad37201b3f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['source', 'author', 'title', 'description', 'url', 'urlToImage',\n",
      "       'publishedAt', 'content'],\n",
      "      dtype='object')\n",
      "                                     source                            author  \\\n",
      "0    {'id': 'bbc-news', 'name': 'BBC News'}  https://www.facebook.com/bbcnews   \n",
      "1  {'id': 'the-verge', 'name': 'The Verge'}                   Jess Weatherbed   \n",
      "2  {'id': 'the-verge', 'name': 'The Verge'}                     Justine Calma   \n",
      "3  {'id': 'the-verge', 'name': 'The Verge'}                     Justine Calma   \n",
      "4          {'id': 'wired', 'name': 'Wired'}                 Stephen Armstrong   \n",
      "\n",
      "                                               title  \\\n",
      "0  Climate change may explain rare dragonfly's sp...   \n",
      "1                  Paris votes to crack down on SUVs   \n",
      "2  The threat of extinction is getting worse for ...   \n",
      "3  NASA’s new mission will study microscopic plan...   \n",
      "4  Climate Finance Is Targeting the Wrong Industries   \n",
      "\n",
      "                                         description  \\\n",
      "0  Experts say rising temperatures may have encou...   \n",
      "1  Parisians have voted to increase parking charg...   \n",
      "2  A new United Nations report gives the fullest ...   \n",
      "3  A new NASA mission will study microscopic plan...   \n",
      "4  Roughly half of the world’s emissions currentl...   \n",
      "\n",
      "                                                 url  \\\n",
      "0     https://www.bbc.co.uk/news/uk-england-68406076   \n",
      "1  https://www.theverge.com/2024/2/5/24061970/suv...   \n",
      "2  https://www.theverge.com/2024/2/13/24071713/mi...   \n",
      "3  https://www.theverge.com/2024/2/8/24066014/nas...   \n",
      "4  https://www.wired.com/story/climate-finance-wr...   \n",
      "\n",
      "                                          urlToImage           publishedAt  \\\n",
      "0  https://ichef.bbci.co.uk/news/1024/branded_new...  2024-02-27T06:34:39Z   \n",
      "1  https://cdn.vox-cdn.com/thumbor/OstemqLr6gLbCl...  2024-02-05T12:24:14Z   \n",
      "2  https://cdn.vox-cdn.com/thumbor/h5kNb06_KICKtr...  2024-02-13T20:00:51Z   \n",
      "3  https://cdn.vox-cdn.com/thumbor/oQ-lJOR2I3m-NL...  2024-02-08T19:12:39Z   \n",
      "4  https://media.wired.com/photos/65caaab26ec7723...  2024-02-13T13:50:49Z   \n",
      "\n",
      "                                             content  \n",
      "0  A once-rare type of dragonfly may have been ai...  \n",
      "1  Paris votes to crack down on SUVs\\r\\nParis vot...  \n",
      "2  The endangered Steppe Eagle is one of the migr...  \n",
      "3  NASAs new mission will study microscopic plank...  \n",
      "4  To achieve net-zero carbon emissions by 2030, ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the list of articles into a DataFrame\n",
    "news_df = pd.DataFrame(articles)\n",
    "\n",
    "# Check the columns and preview the DataFrame \n",
    "print(news_df.columns)\n",
    "print(news_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b63e2-019c-4562-b2e6-e1d6071f130f",
   "metadata": {},
   "source": [
    "Took the list of articles fetched from the News API and converted it into a pandas DataFrame to facilitate data manipulation and analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9f157c-5bf4-4f55-9b00-e7a4115cdaa6",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe252c7f-58bc-4843-8f9e-b5dd436d9660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Start the vectorizer with reasonable parameters to handle large text data\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=10000)\n",
    "\n",
    "# Apply the vectorizer to each dataset\n",
    "twitter_tfidf = tfidf_vectorizer.fit_transform(twitter_df['cleaned_text'])\n",
    "reddit_tfidf = tfidf_vectorizer.transform(reddit_sample_df['cleaned_text'])\n",
    "news_tfidf = tfidf_vectorizer.transform(news_df['content'])  \n",
    "\n",
    "# Dimensionality reduction \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=500, random_state=42)\n",
    "twitter_reduced = svd.fit_transform(twitter_tfidf)\n",
    "reddit_reduced = svd.transform(reddit_tfidf)\n",
    "news_reduced = svd.transform(news_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b0054fa-d13e-4a88-98fc-f54f5b8c4b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter TF-IDF features shape: (43943, 10000)\n",
      "Reddit TF-IDF features shape: (30374, 10000)\n",
      "News TF-IDF features shape: (20, 10000)\n",
      "Twitter reduced features shape: (43943, 500)\n",
      "Reddit reduced features shape: (30374, 500)\n",
      "News reduced features shape: (20, 500)\n"
     ]
    }
   ],
   "source": [
    "# Check shape of transformation after applying TF-IDF vectorization\n",
    "print(\"Twitter TF-IDF features shape:\", twitter_tfidf.shape)\n",
    "print(\"Reddit TF-IDF features shape:\", reddit_tfidf.shape)\n",
    "print(\"News TF-IDF features shape:\", news_tfidf.shape)\n",
    "\n",
    "# Check shape of transformation after applying dimensionality reduction\n",
    "print(\"Twitter reduced features shape:\", twitter_reduced.shape)\n",
    "print(\"Reddit reduced features shape:\", reddit_reduced.shape)\n",
    "print(\"News reduced features shape:\", news_reduced.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc43710f-c6e1-43f6-a3f1-f9174ad63b23",
   "metadata": {},
   "source": [
    "Performed feature engineering on the cleaned text data from Twitter, Reddit, and News articles. TF-IDF vectorization is applied to transform the text data into a numerical format for machine learning, using a vectorizer initialized with specific parameters to handle large datasets. Then, dimensionality reduction is performed using Truncated Singular Value Decomposition (SVD) to reduce the feature space to 500 components to improve the efficiency of the subsequent modeling process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2de36c-5b5d-4ef3-9348-852ddccd7558",
   "metadata": {},
   "source": [
    "### TWITTER DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0006b01b-6f50-4ee9-a864-4a6a64d59e4d",
   "metadata": {},
   "source": [
    "#### Data Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e212e5a-898a-45d5-95cf-c4015f6815bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_label\n",
       "unknown      37980\n",
       "believer      4698\n",
       "skeptic        929\n",
       "undecided      336\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redefine the keywords for labeling\n",
    "believer_keywords = [\"support\", \"agree\", \"pro\"]\n",
    "skeptic_keywords = [\"doubt\", \"deny\", \"skeptic\"]\n",
    "undecided_keywords = [\"unsure\", \"question\", \"undecided\"]\n",
    "\n",
    "# Define the labeling function\n",
    "def label_sentiment(text):\n",
    "    text = text.lower()\n",
    "    if any(keyword in text for keyword in believer_keywords):\n",
    "        return 'believer'\n",
    "    elif any(keyword in text for keyword in skeptic_keywords):\n",
    "        return 'skeptic'\n",
    "    elif any(keyword in text for keyword in undecided_keywords):\n",
    "        return 'undecided'\n",
    "    else:\n",
    "        return 'unknown'  # For tweets that do not contain any of the keywords\n",
    "\n",
    "# Load the Twitter dataset\n",
    "twitter_df = pd.read_csv('D:/ALPHA/Dynamic Folder/Bellevue/Winter 2023/Data Mining/Project/Datasets/Kaggle/twitter_sentiment_data.csv')\n",
    "\n",
    "# Apply the labeling function to the 'message' column\n",
    "twitter_df['sentiment_label'] = twitter_df['message'].apply(label_sentiment)\n",
    "\n",
    "# Display the distribution of the labeled sentiments\n",
    "twitter_df['sentiment_label'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56122559-f71d-4210-8078-015e0cf9b54d",
   "metadata": {},
   "source": [
    "Demonstrated automated data labeling for sentiment analysis. Defined “label_sentiment” as a function to categorize tweets into “believer”, “skeptic”, or “undecided'”, based on the presence of predefined keywords in the text. After applying this function to the “message” column of the Twitter dataset, the distribution of the sentiment labels is displayed, showing the majority of tweets as “unknown”, indicating that they do not contain any of the specified keywords for the three sentiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d5d1a-6a9a-4b63-8eb8-0c1d3e5f3416",
   "metadata": {},
   "source": [
    "#### Model Training using Logistic Regression & Subsequent Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c0e8081-ab3d-4491-8413-eb6e7d763154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9337803855825649,\n",
       " '              precision    recall  f1-score   support\\n\\n    believer       0.93      0.99      0.96       957\\n     skeptic       0.91      0.73      0.81       176\\n   undecided       0.98      0.68      0.80        60\\n\\n    accuracy                           0.93      1193\\n   macro avg       0.94      0.80      0.86      1193\\nweighted avg       0.93      0.93      0.93      1193\\n')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Filter out 'unknown' labeled data\n",
    "filtered_df = twitter_df[twitter_df['sentiment_label'] != 'unknown']\n",
    "\n",
    "# Vectorize the text using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(filtered_df['message'])\n",
    "\n",
    "# Labels\n",
    "y = filtered_df['sentiment_label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "accuracy, report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4fb493-25d8-4672-a3f9-4b76510e3f30",
   "metadata": {},
   "source": [
    "Demonstrated the training of a Logistic Regression model for sentiment classification. Filtered the Twitter dataset to exclude tweets labeled as “unknown”, vectorized the text using TF-IDF, and then split the data into training and test sets. The Logistic Regression model is trained on the vectorized training data, and its performance is evaluated on the test set, showing high accuracy and a detailed classification report with precision, recall, and F1-score metrics for each sentiment category. This evaluation indicates the model's effectiveness in classifying sentiments as “believer” or “skeptic”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6db613-a293-4778-9a87-a54b2422b8c1",
   "metadata": {},
   "source": [
    "#### Iterative Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ca5beb6-12ad-429d-92e3-bd98798acbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9854363408806462,\n",
       " '              precision    recall  f1-score   support\\n\\n    believer       0.99      1.00      0.99      8531\\n     skeptic       0.91      0.56      0.69       190\\n   undecided       1.00      0.50      0.67        68\\n\\n    accuracy                           0.99      8789\\n   macro avg       0.97      0.69      0.78      8789\\nweighted avg       0.98      0.99      0.98      8789\\n')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the dataset to include only 'unknown' labeled tweets for prediction\n",
    "unknown_df = twitter_df.loc[twitter_df['sentiment_label'] == 'unknown']\n",
    "\n",
    "# Vectorize the 'unknown' tweets using the fitted TF-IDF vectorizer\n",
    "X_unknown_tfidf = tfidf_vectorizer.transform(unknown_df['message'])\n",
    "\n",
    "# Predict sentiments for the 'unknown' tweets\n",
    "y_unknown_pred = lr_model.predict(X_unknown_tfidf)\n",
    "\n",
    "# Add the predicted labels back to the unknown_df DataFrame \n",
    "unknown_df.loc[:, 'sentiment_label'] = y_unknown_pred\n",
    "\n",
    "# Combine the originally labeled data with the newly labeled data\n",
    "augmented_df = pd.concat([filtered_df, unknown_df])\n",
    "\n",
    "# Re-vectorize the text of the augmented dataset\n",
    "X_augmented_tfidf = tfidf_vectorizer.transform(augmented_df['message'])\n",
    "y_augmented = augmented_df['sentiment_label']\n",
    "\n",
    "# Re-split the augmented dataset into training and testing sets\n",
    "X_train_augmented, X_test_augmented, y_train_augmented, y_test_augmented = train_test_split(\n",
    "    X_augmented_tfidf, y_augmented, test_size=0.2, random_state=42)\n",
    "\n",
    "# Re-train the Logistic Regression model on the augmented dataset\n",
    "lr_model.fit(X_train_augmented, y_train_augmented)\n",
    "\n",
    "# Predict on the augmented test set and evaluate the model again\n",
    "y_pred_augmented = lr_model.predict(X_test_augmented)\n",
    "accuracy_augmented = accuracy_score(y_test_augmented, y_pred_augmented)\n",
    "report_augmented = classification_report(y_test_augmented, y_pred_augmented)\n",
    "\n",
    "accuracy_augmented, report_augmented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b33c1c9-d876-454b-9bcf-5ceb2314f428",
   "metadata": {},
   "source": [
    "Utilized an iterative refinement process for a sentiment analysis model. Isolated tweets labeled as “unknown” and predicted their sentiments using the previously trained Logistic Regression model. These newly predicted labels are then added back into the dataset, effectively augmenting the training data with high-confidence predictions. The dataset is re-vectorized, split into new training and test sets, and the model is re-trained on this augmented dataset. Finally, the refined model is evaluated to assess improvements in accuracy and other performance metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a5baae-1537-444d-a925-a24fdf583108",
   "metadata": {},
   "source": [
    "### REDDIT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634e3bbd-2ce1-4600-bdc9-b686d0bb3a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the text cleaning function to the 'self_text' column of the Reddit dataset\n",
    "reddit_df['cleaned_text'] = reddit_df['self_text'].apply(clean_text)\n",
    "\n",
    "# Apply the labeling function to the 'cleaned_text' column\n",
    "reddit_df['sentiment_label'] = reddit_df['cleaned_text'].apply(label_sentiment)\n",
    "\n",
    "# Filter out 'unknown' labeled data\n",
    "filtered_reddit_df = reddit_df[reddit_df['sentiment_label'] != 'unknown']\n",
    "\n",
    "# Vectorize the text using TF-IDF\n",
    "X_reddit_tfidf = tfidf_vectorizer.transform(filtered_reddit_df['cleaned_text'])\n",
    "\n",
    "# Labels\n",
    "y_reddit = filtered_reddit_df['sentiment_label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train_reddit, X_test_reddit, y_train_reddit, y_test_reddit = train_test_split(X_reddit_tfidf, y_reddit, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "lr_model_reddit = LogisticRegression(max_iter=1000)\n",
    "lr_model_reddit.fit(X_train_reddit, y_train_reddit)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_reddit = lr_model_reddit.predict(X_test_reddit)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_reddit = accuracy_score(y_test_reddit, y_pred_reddit)\n",
    "report_reddit = classification_report(y_test_reddit, y_pred_reddit)\n",
    "\n",
    "# Iterative Refinement\n",
    "# Predict sentiments for 'unknown' labeled Reddit comments\n",
    "unknown_reddit_df = reddit_df[reddit_df['sentiment_label'] == 'unknown']\n",
    "X_unknown_reddit_tfidf = tfidf_vectorizer.transform(unknown_reddit_df['cleaned_text'])\n",
    "y_unknown_reddit_pred = lr_model_reddit.predict(X_unknown_reddit_tfidf)\n",
    "\n",
    "# Add the predicted labels back to the unknown_reddit_df DataFrame\n",
    "unknown_reddit_df.loc[:, 'sentiment_label'] = y_unknown_reddit_pred\n",
    "\n",
    "# Combine the originally labeled data with the newly labeled data\n",
    "augmented_reddit_df = pd.concat([filtered_reddit_df, unknown_reddit_df])\n",
    "\n",
    "# Re-vectorize the text of the augmented dataset\n",
    "X_augmented_reddit_tfidf = tfidf_vectorizer.transform(augmented_reddit_df['cleaned_text'])\n",
    "y_augmented_reddit = augmented_reddit_df['sentiment_label']\n",
    "\n",
    "# Re-split the augmented dataset into training and testing sets\n",
    "X_train_augmented_reddit, X_test_augmented_reddit, y_train_augmented_reddit, y_test_augmented_reddit = train_test_split(X_augmented_reddit_tfidf, y_augmented_reddit, test_size=0.2, random_state=42)\n",
    "\n",
    "# Re-train the Logistic Regression model on the augmented dataset\n",
    "lr_model_reddit.fit(X_train_augmented_reddit, y_train_augmented_reddit)\n",
    "\n",
    "# Predict on the augmented test set and evaluate the model again\n",
    "y_pred_augmented_reddit = lr_model_reddit.predict(X_test_augmented_reddit)\n",
    "accuracy_augmented_reddit = accuracy_score(y_test_augmented_reddit, y_pred_augmented_reddit)\n",
    "report_augmented_reddit = classification_report(y_test_augmented_reddit, y_pred_augmented_reddit)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Initial model accuracy on Reddit dataset:\", accuracy_reddit)\n",
    "print(\"Initial model classification report on Reddit dataset:\\n\", report_reddit)\n",
    "print(\"Refined model accuracy on Reddit dataset:\", accuracy_augmented_reddit)\n",
    "print(\"Refined model classification report on Reddit dataset:\\n\", report_augmented_reddit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ac4398-ba4b-430f-a2c3-8059fbe9cb9e",
   "metadata": {},
   "source": [
    "For the Reddit dataset, replicated the process applied to the Twitter data, starting with cleaning the text using a predefined function and labeling sentiments based on keyword matching. Then, trained a Logistic Regression model on the vectorized text, evaluated its performance, and used iterative refinement to incorporate high-confidence predictions back into the training set, with the aim of enhancing the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc871d-1cc8-43f7-aa75-493025e8fed8",
   "metadata": {},
   "source": [
    "### API DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78afee2b-5dd6-4cc0-9157-f2640ec721ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Labeling for News API dataset\n",
    "news_df['sentiment_label'] = news_df['content'].apply(label_sentiment)\n",
    "\n",
    "# Filter out 'unknown' labeled data\n",
    "filtered_news_df = news_df[news_df['sentiment_label'] != 'unknown']\n",
    "\n",
    "# Vectorize the text using TF-IDF\n",
    "X_news_tfidf = tfidf_vectorizer.transform(filtered_news_df['content'])\n",
    "\n",
    "# Labels\n",
    "y_news = filtered_news_df['sentiment_label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(X_news_tfidf, y_news, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "# Note: We can use the same model trained on Twitter data or initialize a new one if needed\n",
    "lr_model_news = LogisticRegression(max_iter=1000)\n",
    "lr_model_news.fit(X_train_news, y_train_news)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_news = lr_model_news.predict(X_test_news)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_news = accuracy_score(y_test_news, y_pred_news)\n",
    "report_news = classification_report(y_test_news, y_pred_news)\n",
    "print(accuracy_news, report_news)\n",
    "\n",
    "# Iterative Refinement\n",
    "# Predict sentiments for 'unknown' labeled News articles\n",
    "unknown_news_df = news_df[news_df['sentiment_label'] == 'unknown']\n",
    "X_unknown_news_tfidf = tfidf_vectorizer.transform(unknown_news_df['content'])\n",
    "y_unknown_news_pred = lr_model_news.predict(X_unknown_news_tfidf)\n",
    "\n",
    "# Add the predicted Labels back to the unknown_news_df DataFrame\n",
    "unknown_news_df.loc[:, 'sentiment_label'] = y_unknown_news_pred\n",
    "\n",
    "# Combine the originally labeled data with the newly labeled data\n",
    "augmented_news_df = pd.concat([filtered_news_df, unknown_news_df])\n",
    "\n",
    "# Re-vectorize the text of the augmented dataset\n",
    "X_augmented_news_tfidf = tfidf_vectorizer.transform(augmented_news_df['content'])\n",
    "y_augmented_news = augmented_news_df['sentiment_label']\n",
    "\n",
    "# Re-split the augmented dataset into training and testing sets\n",
    "X_train_augmented_news, X_test_augmented_news, y_train_augmented_news, y_test_augmented_news = train_test_split(X_augmented_news_tfidf, y_augmented_news, test_size=0.2, random_state=42)\n",
    "\n",
    "# Re-train the Logistic Regression model on the augmented dataset\n",
    "lr_model_news.fit(X_train_augmented_news, y_train_augmented_news)\n",
    "\n",
    "# Predict on the augmented test set and evaluate the model again\n",
    "y_pred_augmented_news = lr_model_news.predict(X_test_augmented_news)\n",
    "accuracy_augmented_news = accuracy_score(y_test_augmented_news, y_pred_augmented_news)\n",
    "report_augmented_reddit = classification_report(y_test_augmented_news, y_pred_augmented_news)\n",
    "print(accuracy_augmented_news, report_augmented_news)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee3257a-4703-4e34-a8c8-3161ba55a5fe",
   "metadata": {},
   "source": [
    "Similarly, for the News API dataset, followed the same methodology: cleaned and labeled the articles, vectorized the content, and trained a Logistic Regression model for sentiment classification. Then, evaluated the model's initial performance and applied iterative refinement to improve its predictive capabilities, leveraging the model's own confident classifications to augment the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab5dad0-f431-484c-a32d-b5eee302ce3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
