{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d15090a-3729-4bba-a5a7-e49cd6008164",
   "metadata": {},
   "source": [
    "#### Week 8 Milestone 3 Author: Rex Gayas Course & Section: DSC360-T301 Data Mining: Text Analytics an (2243-1) Date: 04 FEB 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c038b50-26a3-44be-be28-8a980088866a",
   "metadata": {},
   "source": [
    "##### Prepare Data Extraction and Preliminary Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de48998f-3386-4d5c-8258-2849ddf625b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment                                            message  \\\n",
      "0         -1  @tiniebeany climate change is an interesting h...   \n",
      "1          1  RT @NatGeoChannel: Watch #BeforeTheFlood right...   \n",
      "2          1  Fabulous! Leonardo #DiCaprio's film on #climat...   \n",
      "3          1  RT @Mick_Fanning: Just watched this amazing do...   \n",
      "4          2  RT @cnalive: Pranita Biswasi, a Lutheran from ...   \n",
      "\n",
      "              tweetid  \n",
      "0  792927353886371840  \n",
      "1  793124211518832641  \n",
      "2  793124402388832256  \n",
      "3  793124635873275904  \n",
      "4  793125156185137153  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'D:/ALPHA/Dynamic Folder/Bellevue/Winter 2023/Data Mining/Project/Datasets/Kaggle/twitter_sentiment_data.csv'\n",
    "tweets_df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(tweets_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e3ddd46-9a8a-4c6c-adf5-3785bad6c0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentiment', 'message', 'tweetid'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print out the column names to verify the correct column names to avoid possible errors later\n",
    "print(tweets_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9c43cd-9071-4b4a-9bf0-6cf241c6d23c",
   "metadata": {},
   "source": [
    "##### Corpus Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fecc4a8-d43d-4b7a-84a3-d09cb2cae311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             message  \\\n",
      "0  @tiniebeany climate change is an interesting h...   \n",
      "1  RT @NatGeoChannel: Watch #BeforeTheFlood right...   \n",
      "2  Fabulous! Leonardo #DiCaprio's film on #climat...   \n",
      "3  RT @Mick_Fanning: Just watched this amazing do...   \n",
      "4  RT @cnalive: Pranita Biswasi, a Lutheran from ...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  climate change interesting hustle global warmi...  \n",
      "1  rt : watch beforetheflood right , travels worl...  \n",
      "2  fabulous ! leonardo dicaprio 's film climate c...  \n",
      "3  rt : watched amazing documentary leonardodicap...  \n",
      "4  rt : pranita biswasi , lutheran odisha , gives...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Set of NLTK stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_tweet(message):\n",
    "    # Convert to lowercase\n",
    "    message = message.lower()\n",
    "    # Remove URLs\n",
    "    message = re.sub(r'http\\S+|www\\S+|https\\S+', '', message, flags=re.MULTILINE)\n",
    "    # Remove mentions and hashtags\n",
    "    message = re.sub(r'\\@\\w+|\\#','', message)\n",
    "    # Tokenize the message and remove stop words\n",
    "    message = ' '.join([word for word in word_tokenize(message) if word not in stop_words])\n",
    "    return message\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"D:\\\\ALPHA\\\\Dynamic Folder\\\\Bellevue\\\\Winter 2023\\\\Data Mining\\\\Project\\\\Datasets\\\\Kaggle\\\\twitter_sentiment_data.csv\"\n",
    "tweets_df = pd.read_csv(file_path)\n",
    "\n",
    "# Apply the cleaning function to the message text\n",
    "tweets_df['cleaned_text'] = tweets_df['message'].apply(clean_tweet)\n",
    "\n",
    "# Verify the cleaned text \n",
    "print(tweets_df[['message', 'cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56194ace-cf07-4d65-96a2-884d38b7c5f7",
   "metadata": {},
   "source": [
    "##### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f074a042-6e0d-4a3d-865c-c3bbb56f132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['00' '000' '00005' '000058' '000s' '000yr' '001' '00322' '005c' '00pm']\n",
      "Bag of Words Feature Shape: (43943, 31523)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Start the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(tweets_df['cleaned_text'])\n",
    "\n",
    "# Feature names\n",
    "print(\"Feature names:\", vectorizer.get_feature_names_out()[:10])  # print first 10 feature names for verification\n",
    "# Feature matrix shape\n",
    "print(\"Bag of Words Feature Shape:\", X_bow.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc436adb-c72d-43b2-bb4a-40dc75fb5c76",
   "metadata": {},
   "source": [
    "##### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80b6283e-03af-4814-b577-733c3fbd05d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['00' '000' '00005' '000058' '000s' '000yr' '001' '00322' '005c' '00pm']\n",
      "TF-IDF Feature Shape: (43943, 31523)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Start TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(tweets_df['cleaned_text'])\n",
    "\n",
    "# Feature names\n",
    "print(\"Feature names:\", tfidf_vectorizer.get_feature_names_out()[:10])  # print first 10 feature names for verification\n",
    "# Feature matrix shape\n",
    "print(\"TF-IDF Feature Shape:\", X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c7952-2b45-4230-ad8a-4d6fb3bf9e26",
   "metadata": {},
   "source": [
    "##### Bag of N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18a6440a-adf2-4e4d-b619-061e2279ac28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['00' '00 19' '00 19 30' '00 appropriate' '00 appropriate culture'\n",
      " '00 gets' '00 gets hotter' '00 hrs' '00 hrs stockholmact' '00 pm']\n",
      "Bag of N-Grams Feature Shape: (43943, 467247)\n"
     ]
    }
   ],
   "source": [
    "# Start CountVectorizer with N-Grams\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 3))  # Using unigrams, bigrams, and trigrams\n",
    "X_ngram = ngram_vectorizer.fit_transform(tweets_df['cleaned_text'])\n",
    "\n",
    "# Feature names\n",
    "print(\"Feature names:\", ngram_vectorizer.get_feature_names_out()[:10])  # print first 10 feature names for verification\n",
    "# Feature matrix shape\n",
    "print(\"Bag of N-Grams Feature Shape:\", X_ngram.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02d8cbc-478a-4c1b-b67d-128f0e36f971",
   "metadata": {},
   "source": [
    "##### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecc7ac2e-d664-4771-a7da-1ae73d73d6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: climate change rt believe amp going world die thinking fight\n",
      "Topic #1: climate change rt trump action threat planet national via future\n",
      "Topic #2: climate change rt trump us epa new pruitt scott ðÿ\n",
      "Topic #3: global warming rt real believe people say like weather man\n",
      "Topic #4: climate change rt trump via scientists real amp hoax science\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Quantify the number of topics to discover\n",
    "n_topics = 5\n",
    "\n",
    "# Use CountVectorizer's output since LDA works with integer counts, the BoW representation\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda.fit(X_bow)  # X_bow is from the Bag of Words model\n",
    "\n",
    "# Function to print the top words for each topic\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "# Print the top words for each topic\n",
    "print_top_words(lda, vectorizer.get_feature_names_out(), 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2375fcee-8848-4a8e-bd2c-a43efb6bfd09",
   "metadata": {},
   "source": [
    "##### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60c67b0d-47fb-4db3-820f-0d0ff189aa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RexAr\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster\n",
      "4    28547\n",
      "2     9524\n",
      "3     4267\n",
      "0      880\n",
      "1      725\n",
      "Name: count, dtype: int64\n",
      "Top terms per cluster:\n",
      "Cluster 0: epa pruitt scott chief head dioxide primary carbon rt contributor\n",
      "Cluster 1: going husband thinking believe die rt tã mr millions elect\n",
      "Cluster 2: warming global rt real believe think like us people weather\n",
      "Cluster 3: trump climate change rt donald china hoax thinks president via\n",
      "Cluster 4: climate change rt amp world via us real new fight\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Number of clusters chosen\n",
    "n_clusters = 5  \n",
    "\n",
    "# Perform K-Means clustering on the TF-IDF data\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(X_tfidf)\n",
    "\n",
    "# Assign tweets to clusters\n",
    "tweets_df['cluster'] = kmeans.labels_\n",
    "\n",
    "# Check the size of each cluster\n",
    "print(tweets_df['cluster'].value_counts())\n",
    "\n",
    "# Examin cluster centroids\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "for i in range(n_clusters):\n",
    "    top_ten_words = [terms[ind] for ind in order_centroids[i, :10]]\n",
    "    print(\"Cluster {}: {}\".format(i, ' '.join(top_ten_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a2c19-786f-4737-9cc7-31eb7ef2a63b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
