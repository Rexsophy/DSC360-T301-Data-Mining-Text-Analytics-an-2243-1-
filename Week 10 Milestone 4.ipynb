{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "829fac6b-113d-4746-8600-1dd549a25726",
   "metadata": {},
   "source": [
    "#### Week 10 Milestone 4 Author: Rex Gayas Course & Section: DSC360-T301 Data Mining: Text Analytics an (2243-1) Date: 18 FEB 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c86498-339c-4328-9c63-d565795b28a6",
   "metadata": {},
   "source": [
    "#### Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c180ccef-8e62-40cb-b631-c749d88f1d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment                                            message  \\\n",
      "0         -1  @tiniebeany climate change is an interesting h...   \n",
      "1          1  RT @NatGeoChannel: Watch #BeforeTheFlood right...   \n",
      "2          1  Fabulous! Leonardo #DiCaprio's film on #climat...   \n",
      "3          1  RT @Mick_Fanning: Just watched this amazing do...   \n",
      "4          2  RT @cnalive: Pranita Biswasi, a Lutheran from ...   \n",
      "\n",
      "              tweetid                                       cleaned_text  \n",
      "0  792927353886371840  climate change interesting hustle global warmi...  \n",
      "1  793124211518832641  rt : watch beforetheflood right , travels worl...  \n",
      "2  793124402388832256  fabulous ! leonardo dicaprio 's film climate c...  \n",
      "3  793124635873275904  rt : watched amazing documentary leonardodicap...  \n",
      "4  793125156185137153  rt : pranita biswasi , lutheran odisha , gives...  \n",
      "       comment_id  score                                          self_text  \\\n",
      "268628    k14k1rf     16  They are taking the moral stance without putti...   \n",
      "273207    k0b6d9w      6  ...these ambulance chasers, if there's mysery ...   \n",
      "75088     khqn0hw      2  Honestly, still surprises me! Easy mistake to ...   \n",
      "275502    jzuxgzt      5  Does your timeline take into account feedback ...   \n",
      "146191    kcvb571      4             With a cow doing methane measurements.   \n",
      "\n",
      "              subreddit         created_time  post_id           author_name  \\\n",
      "268628  climateskeptics  2023-09-18 13:56:28  16lqdo9           KELEVRACMDR   \n",
      "273207  climateskeptics  2023-09-12 20:37:42  16gzitu  Illustrious_Pepper46   \n",
      "75088     unitedkingdom  2024-01-13 23:41:25  195madt        oxygenthievery   \n",
      "275502    climatechange  2023-09-09 19:58:49  16e8suv           Villager723   \n",
      "146191           canada  2023-12-11 05:39:55  18fkuh3      Distinct-Lynx300   \n",
      "\n",
      "        controversiality  ups  downs  ...  user_comment_karma  \\\n",
      "268628                 0   16      0  ...              3547.0   \n",
      "273207                 0    6      0  ...             12485.0   \n",
      "75088                  0    2      0  ...              2623.0   \n",
      "275502                 0    5      0  ...             69074.0   \n",
      "146191                 0    4      0  ...              4120.0   \n",
      "\n",
      "       user_total_karma  post_score  \\\n",
      "268628           4058.0         174   \n",
      "273207          14968.0          13   \n",
      "75088            2735.0         147   \n",
      "275502          71197.0          25   \n",
      "146191           7256.0          22   \n",
      "\n",
      "                                           post_self_text  \\\n",
      "268628                                                NaN   \n",
      "273207                                                NaN   \n",
      "75088                                                 NaN   \n",
      "275502  Hi,\\n\\nI hate summer, I hate heat, I hate suns...   \n",
      "146191                                                NaN   \n",
      "\n",
      "                                               post_title  post_upvote_ratio  \\\n",
      "268628            She drank the \"Climate Crisis\" Kool-Aid               0.88   \n",
      "273207  Libya floods wipe out quarter of city, death t...               0.79   \n",
      "75088   More than 1km under the North Sea is a climate...               0.92   \n",
      "275502               Where will it still be wet and cool?               0.86   \n",
      "146191  Canada Tackles Methane Emissions From Cow Burp...               0.58   \n",
      "\n",
      "        post_thumbs_ups  post_total_awards_received    post_created_time  \\\n",
      "268628              174                           0  2023-09-18 09:05:20   \n",
      "273207               13                           0  2023-09-12 18:49:19   \n",
      "75088               147                           0  2024-01-13 11:47:21   \n",
      "275502               25                           0  2023-09-09 15:39:11   \n",
      "146191               22                           0  2023-12-11 03:10:17   \n",
      "\n",
      "                                             cleaned_text  \n",
      "268628  taking moral stance without putting work . hit...  \n",
      "273207  ... ambulance chasers , 's mysery world , find...  \n",
      "75088   honestly , still surprises ! easy mistake make...  \n",
      "275502  timeline take account feedback loops , methane...  \n",
      "146191                         cow methane measurements .  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Define a function to clean the text\n",
    "def clean_text(text):\n",
    "    # If text is NaN (float), return an empty string\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs, hashtags, mentions, and stopwords\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # Join the tokens back into a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "# Load Twitter dataset\n",
    "twitter_file_path = 'D:/ALPHA\\Dynamic Folder/Bellevue/Winter 2023/Data Mining/Project/Datasets/Kaggle/twitter_sentiment_data.csv'  \n",
    "twitter_df = pd.read_csv(twitter_file_path)\n",
    "twitter_df['cleaned_text'] = twitter_df['message'].apply(clean_text)\n",
    "\n",
    "# Load Reddit dataset and sample 10% of the data\n",
    "reddit_file_path = 'D:/ALPHA/Dynamic Folder/Bellevue/Winter 2023/Data Mining/Project/Datasets/Kaggle/reddit_opinion_climate_change.csv' \n",
    "reddit_df = pd.read_csv(reddit_file_path)\n",
    "reddit_sample_df = reddit_df.sample(frac=0.1, random_state=42)  # Sample 10% of the data\n",
    "reddit_sample_df['cleaned_text'] = reddit_sample_df['self_text'].apply(clean_text)\n",
    "\n",
    "# Display the first few rows to confirm successful loading and cleaning\n",
    "print(twitter_df.head())\n",
    "print(reddit_sample_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b0a60ed-dd93-4f97-8021-9122e5b6c49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     source             author  \\\n",
      "0  {'id': 'the-verge', 'name': 'The Verge'}    Jess Weatherbed   \n",
      "1  {'id': 'the-verge', 'name': 'The Verge'}      Justine Calma   \n",
      "2  {'id': 'the-verge', 'name': 'The Verge'}      Justine Calma   \n",
      "3          {'id': 'wired', 'name': 'Wired'}  Stephen Armstrong   \n",
      "4          {'id': 'wired', 'name': 'Wired'}      Matt Reynolds   \n",
      "\n",
      "                                               title  \\\n",
      "0                  Paris votes to crack down on SUVs   \n",
      "1  The threat of extinction is getting worse for ...   \n",
      "2  NASA’s new mission will study microscopic plan...   \n",
      "3  Climate Finance Is Targeting the Wrong Industries   \n",
      "4  Inside the Beef Industry’s Campaign to Influen...   \n",
      "\n",
      "                                         description  \\\n",
      "0  Parisians have voted to increase parking charg...   \n",
      "1  A new United Nations report gives the fullest ...   \n",
      "2  A new NASA mission will study microscopic plan...   \n",
      "3  Roughly half of the world’s emissions currentl...   \n",
      "4  Big Beef is wooing science teachers with webin...   \n",
      "\n",
      "                                                 url  \\\n",
      "0  https://www.theverge.com/2024/2/5/24061970/suv...   \n",
      "1  https://www.theverge.com/2024/2/13/24071713/mi...   \n",
      "2  https://www.theverge.com/2024/2/8/24066014/nas...   \n",
      "3  https://www.wired.com/story/climate-finance-wr...   \n",
      "4  https://www.wired.com/story/beef-industry-scho...   \n",
      "\n",
      "                                          urlToImage           publishedAt  \\\n",
      "0  https://cdn.vox-cdn.com/thumbor/OstemqLr6gLbCl...  2024-02-05T12:24:14Z   \n",
      "1  https://cdn.vox-cdn.com/thumbor/h5kNb06_KICKtr...  2024-02-13T20:00:51Z   \n",
      "2  https://cdn.vox-cdn.com/thumbor/oQ-lJOR2I3m-NL...  2024-02-08T19:12:39Z   \n",
      "3  https://media.wired.com/photos/65caaab26ec7723...  2024-02-13T13:50:49Z   \n",
      "4  https://media.wired.com/photos/65b7b6656c7028e...  2024-02-01T12:00:00Z   \n",
      "\n",
      "                                             content  \n",
      "0  Paris votes to crack down on SUVs\\r\\nParis vot...  \n",
      "1  The endangered Steppe Eagle is one of the migr...  \n",
      "2  NASAs new mission will study microscopic plank...  \n",
      "3  To achieve net-zero carbon emissions by 2030, ...  \n",
      "4  Science teachers in many states are currently ...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# NewsAPI key\n",
    "api_key = '586380ef164f4b9287f2e14a286cc9ef'\n",
    "\n",
    "# Define the endpoint and parameters\n",
    "url = 'https://newsapi.org/v2/everything'\n",
    "params = {\n",
    "    'q': 'climate change',  # Query for articles mentioning climate change\n",
    "    'apiKey': api_key,\n",
    "    'pageSize': 20,  # Number of articles to return\n",
    "    'page': 1  # Page number \n",
    "}\n",
    "\n",
    "# Make the request to the NewsAPI\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    articles = response.json()['articles']\n",
    "    # Create a DataFrame with the relevant information\n",
    "    df_articles = pd.DataFrame(articles)\n",
    "    print(df_articles.head())  # Display the first few entries\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89abe746-6aa0-4464-a59d-ad37201b3f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['source', 'author', 'title', 'description', 'url', 'urlToImage',\n",
      "       'publishedAt', 'content'],\n",
      "      dtype='object')\n",
      "                                     source             author  \\\n",
      "0  {'id': 'the-verge', 'name': 'The Verge'}    Jess Weatherbed   \n",
      "1  {'id': 'the-verge', 'name': 'The Verge'}      Justine Calma   \n",
      "2  {'id': 'the-verge', 'name': 'The Verge'}      Justine Calma   \n",
      "3          {'id': 'wired', 'name': 'Wired'}  Stephen Armstrong   \n",
      "4          {'id': 'wired', 'name': 'Wired'}      Matt Reynolds   \n",
      "\n",
      "                                               title  \\\n",
      "0                  Paris votes to crack down on SUVs   \n",
      "1  The threat of extinction is getting worse for ...   \n",
      "2  NASA’s new mission will study microscopic plan...   \n",
      "3  Climate Finance Is Targeting the Wrong Industries   \n",
      "4  Inside the Beef Industry’s Campaign to Influen...   \n",
      "\n",
      "                                         description  \\\n",
      "0  Parisians have voted to increase parking charg...   \n",
      "1  A new United Nations report gives the fullest ...   \n",
      "2  A new NASA mission will study microscopic plan...   \n",
      "3  Roughly half of the world’s emissions currentl...   \n",
      "4  Big Beef is wooing science teachers with webin...   \n",
      "\n",
      "                                                 url  \\\n",
      "0  https://www.theverge.com/2024/2/5/24061970/suv...   \n",
      "1  https://www.theverge.com/2024/2/13/24071713/mi...   \n",
      "2  https://www.theverge.com/2024/2/8/24066014/nas...   \n",
      "3  https://www.wired.com/story/climate-finance-wr...   \n",
      "4  https://www.wired.com/story/beef-industry-scho...   \n",
      "\n",
      "                                          urlToImage           publishedAt  \\\n",
      "0  https://cdn.vox-cdn.com/thumbor/OstemqLr6gLbCl...  2024-02-05T12:24:14Z   \n",
      "1  https://cdn.vox-cdn.com/thumbor/h5kNb06_KICKtr...  2024-02-13T20:00:51Z   \n",
      "2  https://cdn.vox-cdn.com/thumbor/oQ-lJOR2I3m-NL...  2024-02-08T19:12:39Z   \n",
      "3  https://media.wired.com/photos/65caaab26ec7723...  2024-02-13T13:50:49Z   \n",
      "4  https://media.wired.com/photos/65b7b6656c7028e...  2024-02-01T12:00:00Z   \n",
      "\n",
      "                                             content  \n",
      "0  Paris votes to crack down on SUVs\\r\\nParis vot...  \n",
      "1  The endangered Steppe Eagle is one of the migr...  \n",
      "2  NASAs new mission will study microscopic plank...  \n",
      "3  To achieve net-zero carbon emissions by 2030, ...  \n",
      "4  Science teachers in many states are currently ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the list of articles into a DataFrame\n",
    "news_df = pd.DataFrame(articles)\n",
    "\n",
    "# Check the columns and preview the DataFrame \n",
    "print(news_df.columns)\n",
    "print(news_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9f157c-5bf4-4f55-9b00-e7a4115cdaa6",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe252c7f-58bc-4843-8f9e-b5dd436d9660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Start the vectorizer with reasonable parameters to handle large text data\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=10000)\n",
    "\n",
    "# Apply the vectorizer to each dataset\n",
    "twitter_tfidf = tfidf_vectorizer.fit_transform(twitter_df['cleaned_text'])\n",
    "reddit_tfidf = tfidf_vectorizer.transform(reddit_sample_df['cleaned_text'])\n",
    "news_tfidf = tfidf_vectorizer.transform(news_df['content'])  \n",
    "\n",
    "# Dimensionality reduction \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=500, random_state=42)\n",
    "twitter_reduced = svd.fit_transform(twitter_tfidf)\n",
    "reddit_reduced = svd.transform(reddit_tfidf)\n",
    "news_reduced = svd.transform(news_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b0054fa-d13e-4a88-98fc-f54f5b8c4b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter TF-IDF features shape: (43943, 10000)\n",
      "Reddit TF-IDF features shape: (30374, 10000)\n",
      "News TF-IDF features shape: (20, 10000)\n",
      "Twitter reduced features shape: (43943, 500)\n",
      "Reddit reduced features shape: (30374, 500)\n",
      "News reduced features shape: (20, 500)\n"
     ]
    }
   ],
   "source": [
    "# Check shape of transformation after applying TF-IDF vectorization\n",
    "print(\"Twitter TF-IDF features shape:\", twitter_tfidf.shape)\n",
    "print(\"Reddit TF-IDF features shape:\", reddit_tfidf.shape)\n",
    "print(\"News TF-IDF features shape:\", news_tfidf.shape)\n",
    "\n",
    "# Check shape of transformation after applying dimensionality reduction\n",
    "print(\"Twitter reduced features shape:\", twitter_reduced.shape)\n",
    "print(\"Reddit reduced features shape:\", reddit_reduced.shape)\n",
    "print(\"News reduced features shape:\", news_reduced.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0006b01b-6f50-4ee9-a864-4a6a64d59e4d",
   "metadata": {},
   "source": [
    "#### Data Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e212e5a-898a-45d5-95cf-c4015f6815bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment_label\n",
       "unknown      37980\n",
       "believer      4698\n",
       "skeptic        929\n",
       "undecided      336\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redefine the keywords for labeling\n",
    "believer_keywords = [\"support\", \"agree\", \"pro\"]\n",
    "skeptic_keywords = [\"doubt\", \"deny\", \"skeptic\"]\n",
    "undecided_keywords = [\"unsure\", \"question\", \"undecided\"]\n",
    "\n",
    "# Define the labeling function\n",
    "def label_sentiment(text):\n",
    "    text = text.lower()\n",
    "    if any(keyword in text for keyword in believer_keywords):\n",
    "        return 'believer'\n",
    "    elif any(keyword in text for keyword in skeptic_keywords):\n",
    "        return 'skeptic'\n",
    "    elif any(keyword in text for keyword in undecided_keywords):\n",
    "        return 'undecided'\n",
    "    else:\n",
    "        return 'unknown'  # For tweets that do not contain any of the keywords\n",
    "\n",
    "# Load the Twitter dataset\n",
    "twitter_df = pd.read_csv('D:/ALPHA/Dynamic Folder/Bellevue/Winter 2023/Data Mining/Project/Datasets/Kaggle/twitter_sentiment_data.csv')\n",
    "\n",
    "# Apply the labeling function to the 'message' column\n",
    "twitter_df['sentiment_label'] = twitter_df['message'].apply(label_sentiment)\n",
    "\n",
    "# Display the distribution of the labeled sentiments\n",
    "twitter_df['sentiment_label'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d5d1a-6a9a-4b63-8eb8-0c1d3e5f3416",
   "metadata": {},
   "source": [
    "#### Model Training using Logistic Regression & Subsequent Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c0e8081-ab3d-4491-8413-eb6e7d763154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9337803855825649,\n",
       " '              precision    recall  f1-score   support\\n\\n    believer       0.93      0.99      0.96       957\\n     skeptic       0.91      0.73      0.81       176\\n   undecided       0.98      0.68      0.80        60\\n\\n    accuracy                           0.93      1193\\n   macro avg       0.94      0.80      0.86      1193\\nweighted avg       0.93      0.93      0.93      1193\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Filter out 'unknown' labeled data\n",
    "filtered_df = twitter_df[twitter_df['sentiment_label'] != 'unknown']\n",
    "\n",
    "# Vectorize the text using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(filtered_df['message'])\n",
    "\n",
    "# Labels\n",
    "y = filtered_df['sentiment_label']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "accuracy, report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6db613-a293-4778-9a87-a54b2422b8c1",
   "metadata": {},
   "source": [
    "#### Iterative Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ca5beb6-12ad-429d-92e3-bd98798acbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9854363408806462,\n",
       " '              precision    recall  f1-score   support\\n\\n    believer       0.99      1.00      0.99      8531\\n     skeptic       0.91      0.56      0.69       190\\n   undecided       1.00      0.50      0.67        68\\n\\n    accuracy                           0.99      8789\\n   macro avg       0.97      0.69      0.78      8789\\nweighted avg       0.98      0.99      0.98      8789\\n')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the dataset to include only 'unknown' labeled tweets for prediction\n",
    "unknown_df = twitter_df.loc[twitter_df['sentiment_label'] == 'unknown']\n",
    "\n",
    "# Vectorize the 'unknown' tweets using the fitted TF-IDF vectorizer\n",
    "X_unknown_tfidf = tfidf_vectorizer.transform(unknown_df['message'])\n",
    "\n",
    "# Predict sentiments for the 'unknown' tweets\n",
    "y_unknown_pred = lr_model.predict(X_unknown_tfidf)\n",
    "\n",
    "# Add the predicted labels back to the unknown_df DataFrame \n",
    "unknown_df.loc[:, 'sentiment_label'] = y_unknown_pred\n",
    "\n",
    "# Combine the originally labeled data with the newly labeled data\n",
    "augmented_df = pd.concat([filtered_df, unknown_df])\n",
    "\n",
    "# Re-vectorize the text of the augmented dataset\n",
    "X_augmented_tfidf = tfidf_vectorizer.transform(augmented_df['message'])\n",
    "y_augmented = augmented_df['sentiment_label']\n",
    "\n",
    "# Re-split the augmented dataset into training and testing sets\n",
    "X_train_augmented, X_test_augmented, y_train_augmented, y_test_augmented = train_test_split(\n",
    "    X_augmented_tfidf, y_augmented, test_size=0.2, random_state=42)\n",
    "\n",
    "# Re-train the Logistic Regression model on the augmented dataset\n",
    "lr_model.fit(X_train_augmented, y_train_augmented)\n",
    "\n",
    "# Predict on the augmented test set and evaluate the model again\n",
    "y_pred_augmented = lr_model.predict(X_test_augmented)\n",
    "accuracy_augmented = accuracy_score(y_test_augmented, y_pred_augmented)\n",
    "report_augmented = classification_report(y_test_augmented, y_pred_augmented)\n",
    "\n",
    "accuracy_augmented, report_augmented\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
