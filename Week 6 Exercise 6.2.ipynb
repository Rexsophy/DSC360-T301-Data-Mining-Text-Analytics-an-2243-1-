{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cbd1e1f",
   "metadata": {},
   "source": [
    "## Assignment 6.2\n",
    "Author: Rex Gayas\n",
    "Date: 20 January 2024\n",
    "Modified By: N/A\n",
    "Description: Aggregating Pandas dataframes and delving into time series analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7779d",
   "metadata": {},
   "source": [
    "##### Solution to Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d683dc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      mag magType           time                         place  tsunami  \\\n",
      "1563  4.9      mb  1538977532250  293km ESE of Iwo Jima, Japan        0   \n",
      "2576  5.4      mb  1538697528010    37km E of Tomakomai, Japan        0   \n",
      "3072  4.9      mb  1538579732490     15km ENE of Hasaki, Japan        0   \n",
      "3632  4.9      mb  1538450871260    53km ESE of Hitachi, Japan        0   \n",
      "\n",
      "     parsed_place  \n",
      "1563        Japan  \n",
      "2576        Japan  \n",
      "3072        Japan  \n",
      "3632        Japan  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the earthquakes data\n",
    "earthquakes_path = \"D:\\\\ALPHA\\\\Dynamic Folder\\\\Bellevue\\\\Winter 2023\\\\Data Wrangling\\\\Week 6\\\\earthquakes.csv\"\n",
    "earthquakes_df = pd.read_csv(earthquakes_path)\n",
    "\n",
    "# Filter for earthquakes in Japan with a magnitude of 4.9 or greater of 'mb' type\n",
    "japan_earthquakes = earthquakes_df[(earthquakes_df['place'].str.contains('Japan')) &\n",
    "                                   (earthquakes_df['magType'] == 'mb') &\n",
    "                                   (earthquakes_df['mag'] >= 4.9)]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(japan_earthquakes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a8bf80",
   "metadata": {},
   "source": [
    "##### Solution to Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "873fb532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mag\n",
      "[-1, 0)     446\n",
      "[0, 1)     2072\n",
      "[1, 2)     3126\n",
      "[2, 3)      985\n",
      "[3, 4)      153\n",
      "[4, 5)        6\n",
      "[5, 6)        2\n",
      "[6, 7)        0\n",
      "[7, 8)        0\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create bins for earthquake magnitude\n",
    "bins = range(int(earthquakes_df['mag'].min()), int(earthquakes_df['mag'].max()) + 2) # +2 to include the last bin\n",
    "ml_earthquakes = earthquakes_df[earthquakes_df['magType'] == 'ml']\n",
    "\n",
    "# Use pd.cut to bin the data and value_counts to count the occurrences\n",
    "magnitude_bins = pd.cut(ml_earthquakes['mag'], bins=bins, include_lowest=True, right=False)\n",
    "bin_counts = magnitude_bins.value_counts().sort_index()\n",
    "\n",
    "# Display the count of earthquakes in each bin\n",
    "print(bin_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb9067",
   "metadata": {},
   "source": [
    "##### Solution to Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00b8a820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ticker       date         open         high          low        close  \\\n",
      "0    AAPL 2018-01-31    43.505357    45.025002    41.174999    43.501309   \n",
      "1    AAPL 2018-02-28    41.819079    45.154999    37.560001    41.909737   \n",
      "2    AAPL 2018-03-31    43.761786    45.875000    41.235001    43.624048   \n",
      "3    AAPL 2018-04-30    42.441310    44.735001    40.157501    42.458572   \n",
      "4    AAPL 2018-05-31    46.239091    47.592499    41.317501    46.384205   \n",
      "5    AAPL 2018-06-30    47.180119    48.549999    45.182499    47.155357   \n",
      "6    AAPL 2018-07-31    47.549048    48.990002    45.855000    47.577857   \n",
      "7    AAPL 2018-08-31    53.121739    57.217499    49.327499    53.336522   \n",
      "8    AAPL 2018-09-30    55.582763    57.417500    53.825001    55.518421   \n",
      "9    AAPL 2018-10-31    55.300000    58.367500    51.522499    55.211413   \n",
      "10   AAPL 2018-11-30    47.954881    55.590000    42.564999    47.808929   \n",
      "11   AAPL 2018-12-31    41.310789    46.235001    36.647499    41.066579   \n",
      "12   AMZN 2018-01-31  1301.377151  1472.579956  1170.510010  1309.010946   \n",
      "13   AMZN 2018-02-28  1447.113159  1528.699951  1265.930054  1442.363146   \n",
      "14   AMZN 2018-03-31  1542.160464  1617.540039  1365.199951  1540.367629   \n",
      "15   AMZN 2018-04-30  1475.841902  1638.099976  1352.880005  1468.220471   \n",
      "16   AMZN 2018-05-31  1590.474543  1635.000000  1546.020020  1594.903637   \n",
      "17   AMZN 2018-06-30  1699.088582  1763.099976  1635.089966  1698.823812   \n",
      "18   AMZN 2018-07-31  1786.305716  1880.050049  1678.060059  1784.649042   \n",
      "19   AMZN 2018-08-31  1891.957833  2025.569946  1776.020020  1897.851308   \n",
      "20   AMZN 2018-09-30  1969.239476  2050.500000  1865.000000  1966.077900   \n",
      "21   AMZN 2018-10-31  1799.630865  2033.189941  1476.359985  1782.058265   \n",
      "22   AMZN 2018-11-30  1622.323806  1784.000000  1420.000000  1625.483823   \n",
      "23   AMZN 2018-12-31  1572.922100  1778.339966  1307.000000  1559.443154   \n",
      "24     FB 2018-01-31   184.584284   190.660004   175.800003   184.962856   \n",
      "25     FB 2018-02-28   180.721578   195.320007   167.179993   180.269473   \n",
      "26     FB 2018-03-31   173.449524   186.100006   149.020004   173.489522   \n",
      "27     FB 2018-04-30   164.163332   177.100006   150.509995   163.810476   \n",
      "28     FB 2018-05-31   181.910909   192.720001   170.229996   182.930000   \n",
      "29     FB 2018-06-30   194.974763   203.550003   186.429993   195.267620   \n",
      "30     FB 2018-07-31   199.332381   218.619995   166.559998   199.967142   \n",
      "31     FB 2018-08-31   177.598695   188.300003   170.270004   177.492172   \n",
      "32     FB 2018-09-30   164.233158   173.889999   158.869995   164.377368   \n",
      "33     FB 2018-10-31   154.873479   165.880005   139.029999   154.187826   \n",
      "34     FB 2018-11-30   141.762857   154.130005   126.849998   141.635715   \n",
      "35     FB 2018-12-31   137.529475   147.190002   123.019997   137.161052   \n",
      "36   GOOG 2018-01-31  1127.200945  1186.890015  1045.229980  1130.770467   \n",
      "37   GOOG 2018-02-28  1088.629472  1174.000000   992.559998  1088.206839   \n",
      "38   GOOG 2018-03-31  1096.108085  1177.050049   980.640015  1091.490479   \n",
      "39   GOOG 2018-04-30  1038.415237  1094.165039   990.369995  1035.696187   \n",
      "40   GOOG 2018-05-31  1064.021376  1110.750000  1006.289978  1069.275901   \n",
      "41   GOOG 2018-06-30  1136.396182  1186.286011  1096.010010  1137.626668   \n",
      "42   GOOG 2018-07-31  1183.464280  1273.890015  1093.800049  1187.590472   \n",
      "43   GOOG 2018-08-31  1226.156951  1256.500000  1188.239990  1225.671732   \n",
      "44   GOOG 2018-09-30  1176.878424  1212.989990  1146.910034  1175.808934   \n",
      "45   GOOG 2018-10-31  1116.082172  1209.959961   995.830017  1110.940411   \n",
      "46   GOOG 2018-11-30  1054.971424  1095.569946   996.020020  1056.162394   \n",
      "47   GOOG 2018-12-31  1042.619998  1124.650024   970.109985  1037.420519   \n",
      "48   NFLX 2018-01-31   231.269525   286.809998   195.419998   232.908096   \n",
      "49   NFLX 2018-02-28   270.873158   297.359985   236.110001   271.443683   \n",
      "50   NFLX 2018-03-31   312.712859   333.980011   275.899994   312.228097   \n",
      "51   NFLX 2018-04-30   309.129524   338.820007   271.220001   307.466192   \n",
      "52   NFLX 2018-05-31   329.779541   356.100006   305.730011   331.536819   \n",
      "53   NFLX 2018-06-30   384.557143   423.209991   352.820007   384.133336   \n",
      "54   NFLX 2018-07-31   380.969526   419.769989   328.000000   381.515238   \n",
      "55   NFLX 2018-08-31   345.410001   376.809998   310.929993   346.257824   \n",
      "56   NFLX 2018-09-30   363.326843   383.200012   335.829987   362.641576   \n",
      "57   NFLX 2018-10-31   340.025218   386.799988   271.209991   335.445652   \n",
      "58   NFLX 2018-11-30   290.643335   332.049988   250.000000   290.344764   \n",
      "59   NFLX 2018-12-31   266.309474   298.720001   231.229996   265.302630   \n",
      "\n",
      "          volume  \n",
      "0   2.638718e+09  \n",
      "1   3.711577e+09  \n",
      "2   2.854911e+09  \n",
      "3   2.664617e+09  \n",
      "4   2.483905e+09  \n",
      "5   2.110498e+09  \n",
      "6   1.574766e+09  \n",
      "7   2.801276e+09  \n",
      "8   2.715888e+09  \n",
      "9   3.158994e+09  \n",
      "10  3.845306e+09  \n",
      "11  3.595690e+09  \n",
      "12  9.637120e+07  \n",
      "13  1.377840e+08  \n",
      "14  1.304001e+08  \n",
      "15  1.299196e+08  \n",
      "16  7.161550e+07  \n",
      "17  8.594130e+07  \n",
      "18  9.752110e+07  \n",
      "19  9.657580e+07  \n",
      "20  9.444550e+07  \n",
      "21  1.832208e+08  \n",
      "22  1.392900e+08  \n",
      "23  1.548127e+08  \n",
      "24  4.956557e+08  \n",
      "25  5.162516e+08  \n",
      "26  9.962017e+08  \n",
      "27  7.500727e+08  \n",
      "28  4.011441e+08  \n",
      "29  3.872656e+08  \n",
      "30  6.470307e+08  \n",
      "31  5.488327e+08  \n",
      "32  5.004688e+08  \n",
      "33  6.224463e+08  \n",
      "34  5.181517e+08  \n",
      "35  5.587862e+08  \n",
      "36  2.873840e+07  \n",
      "37  4.238200e+07  \n",
      "38  4.535330e+07  \n",
      "39  4.171590e+07  \n",
      "40  3.184940e+07  \n",
      "41  3.209600e+07  \n",
      "42  3.194010e+07  \n",
      "43  2.880840e+07  \n",
      "44  2.886240e+07  \n",
      "45  4.849470e+07  \n",
      "46  3.673510e+07  \n",
      "47  4.025760e+07  \n",
      "48  2.383776e+08  \n",
      "49  1.845858e+08  \n",
      "50  2.634494e+08  \n",
      "51  2.620060e+08  \n",
      "52  1.420508e+08  \n",
      "53  2.440318e+08  \n",
      "54  3.053938e+08  \n",
      "55  2.131223e+08  \n",
      "56  1.708321e+08  \n",
      "57  3.635898e+08  \n",
      "58  2.571264e+08  \n",
      "59  2.343100e+08  \n"
     ]
    }
   ],
   "source": [
    "# Load the FAANG data\n",
    "faang_path = \"D:\\\\ALPHA\\\\Dynamic Folder\\\\Bellevue\\\\Winter 2023\\\\Data Wrangling\\\\Week 6\\\\faang.csv\"\n",
    "faang_df = pd.read_csv(faang_path, parse_dates=['date'], index_col='date')\n",
    "\n",
    "# Resample to monthly frequency and perform aggregations\n",
    "monthly_resampled = faang_df.groupby('ticker').resample('M').agg({\n",
    "    'open': 'mean',       # Mean of the opening price\n",
    "    'high': 'max',        # Maximum of the high price\n",
    "    'low': 'min',         # Minimum of the low price\n",
    "    'close': 'mean',      # Mean of the closing price\n",
    "    'volume': 'sum',      # Sum of the volume traded\n",
    "})\n",
    "\n",
    "# Reset index to flatten the DataFrame\n",
    "monthly_resampled = monthly_resampled.reset_index()\n",
    "\n",
    "# Display the resampled DataFrame\n",
    "print(monthly_resampled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c88ed3",
   "metadata": {},
   "source": [
    "##### Solution to Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc56a721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "magType   mb  mb_lg    md   mh   ml  ms_20    mw  mwb  mwr  mww\n",
      "tsunami                                                        \n",
      "0        5.6    3.5  4.11  1.1  4.2    0.0  3.83  5.8  4.8  6.0\n",
      "1        6.1    0.0  0.00  0.0  5.1    5.7  4.41  0.0  0.0  7.5\n"
     ]
    }
   ],
   "source": [
    "# Crosstabulation between 'tsunami' and 'magType' with max magnitude\n",
    "crosstab_result = pd.crosstab(index=earthquakes_df['tsunami'], columns=earthquakes_df['magType'],\n",
    "                              values=earthquakes_df['mag'], aggfunc='max').fillna(0)\n",
    "\n",
    "# Display the crosstab result\n",
    "print(crosstab_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbc26ed",
   "metadata": {},
   "source": [
    "###### Solution to Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c566bb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ticker       date        open        high         low       close  \\\n",
      "0      AAPL 2018-01-02   42.540001   43.075001   42.314999   43.064999   \n",
      "1      AAPL 2018-01-03   42.836250   43.637501   42.314999   43.061249   \n",
      "2      AAPL 2018-01-04   42.935833   43.637501   42.314999   43.126666   \n",
      "3      AAPL 2018-01-05   43.041875   43.842499   42.314999   43.282499   \n",
      "4      AAPL 2018-01-08   43.151000   43.902500   42.314999   43.343500   \n",
      "...     ...        ...         ...         ...         ...         ...   \n",
      "1250   NFLX 2018-12-24  283.509251  332.049988  233.679993  281.931750   \n",
      "1251   NFLX 2018-12-26  281.844501  332.049988  231.229996  280.777750   \n",
      "1252   NFLX 2018-12-27  281.070489  332.049988  231.229996  280.162927   \n",
      "1253   NFLX 2018-12-28  279.916342  332.049988  231.229996  279.461464   \n",
      "1254   NFLX 2018-12-31  278.430770  332.049988  231.229996  277.451539   \n",
      "\n",
      "           volume  \n",
      "0     102223600.0  \n",
      "1     220295200.0  \n",
      "2     310033600.0  \n",
      "3     404673600.0  \n",
      "4     486944800.0  \n",
      "...           ...  \n",
      "1250  525657600.0  \n",
      "1251  520444300.0  \n",
      "1252  532679500.0  \n",
      "1253  521973500.0  \n",
      "1254  476314900.0  \n",
      "\n",
      "[1255 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Rolling 60-day aggregation of OHLC data by ticker for FAANG data\n",
    "rolling_aggregations = faang_df.groupby('ticker').rolling(window='60D').agg({\n",
    "    'open': 'mean',       # Mean of the opening price\n",
    "    'high': 'max',        # Maximum of the high price\n",
    "    'low': 'min',         # Minimum of the low price\n",
    "    'close': 'mean',      # Mean of the closing price\n",
    "    'volume': 'sum',      # Sum of the volume traded\n",
    "}).dropna().reset_index()\n",
    "\n",
    "# Display the rolling aggregations\n",
    "print(rolling_aggregations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c61f51",
   "metadata": {},
   "source": [
    "##### Solution to Exercise 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b01b71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              close         high          low         open        volume\n",
      "ticker                                                                  \n",
      "AAPL      47.263357    47.748526    46.795877    47.277859  1.360803e+08\n",
      "AMZN    1641.726176  1662.839839  1619.840519  1644.072709  5.648994e+06\n",
      "FB       171.510956   173.613347   169.303148   171.472948  2.765860e+07\n",
      "GOOG    1113.225134  1125.777606  1101.001658  1113.554101  1.741965e+06\n",
      "NFLX     319.290319   325.219322   313.187330   319.620558  1.146962e+07\n"
     ]
    }
   ],
   "source": [
    "# Create a pivot table with the ticker as rows and the averages of OHLC and volume\n",
    "faang_pivot = faang_df.pivot_table(\n",
    "    index='ticker', \n",
    "    values=['open', 'high', 'low', 'close', 'volume'], \n",
    "    aggfunc='mean'  # Averages of the specified columns\n",
    ")\n",
    "\n",
    "# Display the pivot table\n",
    "print(faang_pivot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2542c7",
   "metadata": {},
   "source": [
    "##### Solution to Exercise 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "727cab13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                high       low      open     close    volume\n",
      "date                                                        \n",
      "2018-10-01  2.368006  2.502113  2.337813  2.385848 -1.630411\n",
      "2018-10-02  2.227302  2.247433  2.190795  2.155037 -0.861879\n",
      "2018-10-03  2.058955  2.139987  2.068570  2.025489 -0.920345\n",
      "2018-10-04  1.819474  1.781561  1.850048  1.722816 -0.126582\n",
      "2018-10-05  1.628173  1.554416  1.642819  1.584748 -0.298771\n",
      "...              ...       ...       ...       ...       ...\n",
      "2018-12-24 -2.159820 -2.187566 -2.179582 -2.226185 -0.141238\n",
      "2018-12-26 -1.611714 -1.810493 -2.026617 -1.339674  1.123063\n",
      "2018-12-27 -1.641276 -1.626703 -1.456521 -1.404343  0.849827\n",
      "2018-12-28 -1.325261 -1.231588 -1.328549 -1.289951  0.496102\n",
      "2018-12-31 -1.273456 -0.975763 -1.078283 -1.122691 -0.246405\n",
      "\n",
      "[63 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the FAANG data\n",
    "faang_path = \"D:\\\\ALPHA\\\\Dynamic Folder\\\\Bellevue\\\\Winter 2023\\\\Data Wrangling\\\\Week 6\\\\faang.csv\"\n",
    "faang_df = pd.read_csv(faang_path, parse_dates=['date'], index_col='date')\n",
    "\n",
    "# Filter for Amazon's data in Q4 2018\n",
    "amzn_q4_data = faang_df[(faang_df['ticker'] == 'AMZN') & \n",
    "                        (faang_df.index >= '2018-10-01') & \n",
    "                        (faang_df.index <= '2018-12-31')]\n",
    "\n",
    "# Function to calculate Z-score\n",
    "def calculate_z_score(column):\n",
    "    return (column - column.mean()) / column.std()\n",
    "\n",
    "# Calculate Z-scores using apply() on numeric columns\n",
    "amzn_q4_z_scores = amzn_q4_data.select_dtypes(include=[np.number]).apply(calculate_z_score)\n",
    "\n",
    "# Display the Z-scores\n",
    "print(amzn_q4_z_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7bf549",
   "metadata": {},
   "source": [
    "##### Solution to Exercise 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c00582a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          high          low         open        close  \\\n",
      "date       ticker                                                       \n",
      "2018-01-02 AAPL      43.075001    42.314999    42.540001    43.064999   \n",
      "           AMZN    1190.000000  1170.510010  1172.000000  1189.010010   \n",
      "           FB       181.580002   177.550003   177.679993   181.419998   \n",
      "           GOOG    1066.939941  1045.229980  1048.339966  1065.000000   \n",
      "           NFLX     201.649994   195.419998   196.100006   201.070007   \n",
      "...                        ...          ...          ...          ...   \n",
      "2018-12-31 AAPL      39.840000    39.119999    39.632500    39.435001   \n",
      "           AMZN    1520.760010  1487.000000  1510.800049  1501.969971   \n",
      "           FB       134.639999   129.949997   134.449997   131.089996   \n",
      "           GOOG    1052.699951  1023.590027  1050.959961  1035.609985   \n",
      "           NFLX     270.100006   260.000000   260.160004   267.660004   \n",
      "\n",
      "                        volume event  \n",
      "date       ticker                     \n",
      "2018-01-02 AAPL    102223600.0   NaN  \n",
      "           AMZN      2694500.0   NaN  \n",
      "           FB       18151900.0   NaN  \n",
      "           GOOG      1237600.0   NaN  \n",
      "           NFLX     10966900.0   NaN  \n",
      "...                        ...   ...  \n",
      "2018-12-31 AAPL    140014000.0   NaN  \n",
      "           AMZN      6954500.0   NaN  \n",
      "           FB       24625300.0   NaN  \n",
      "           GOOG      1493300.0   NaN  \n",
      "           NFLX     13508900.0   NaN  \n",
      "\n",
      "[1255 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# a) Create a DataFrame with event descriptions for FB\n",
    "events = {\n",
    "    'ticker': 'FB',\n",
    "    'date': pd.to_datetime(['2018-07-25', '2018-03-19', '2018-03-20']),\n",
    "    'event': [\n",
    "        'Disappointing user growth announced after close.',\n",
    "        'Cambridge Analytica story',\n",
    "        'FTC investigation'\n",
    "    ]\n",
    "}\n",
    "events_df = pd.DataFrame(events).set_index(['date', 'ticker'])\n",
    "\n",
    "# Reset index for faang_df to ensure the merge operation can be performed\n",
    "faang_df.reset_index(inplace=True)\n",
    "\n",
    "# Ensure 'date' is of datetime type\n",
    "faang_df['date'] = pd.to_datetime(faang_df['date'])\n",
    "\n",
    "# Set the new multi-index for faang_df\n",
    "faang_df.set_index(['date', 'ticker'], inplace=True)\n",
    "\n",
    "# c) Merge with the FAANG data using an outer join\n",
    "merged_data = faang_df.merge(events_df, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(merged_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2be83c8",
   "metadata": {},
   "source": [
    "##### Solution to Exercise 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe9c2148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       high       low      open     close    volume\n",
      "date       ticker                                                  \n",
      "2018-01-02 FB      1.000000  1.000000  1.000000  1.000000  1.000000\n",
      "2018-01-03 FB      1.017623  1.021290  1.023638  1.017914  0.930294\n",
      "2018-01-04 FB      1.025498  1.036891  1.040635  1.016040  0.764708\n",
      "2018-01-05 FB      1.029298  1.041566  1.044518  1.029931  0.747828\n",
      "2018-01-08 FB      1.040313  1.049451  1.053579  1.037813  0.991340\n",
      "...                     ...       ...       ...       ...       ...\n",
      "2018-12-24 GOOG    0.940578  0.928131  0.928993  0.916638  1.284987\n",
      "2018-12-26 GOOG    0.974750  0.940463  0.943406  0.976019  1.917663\n",
      "2018-12-27 GOOG    0.978396  0.953857  0.970248  0.980169  1.704751\n",
      "2018-12-28 GOOG    0.989334  0.988395  1.001221  0.973784  1.143180\n",
      "2018-12-31 GOOG    0.986653  0.979296  1.002499  0.972404  1.206610\n",
      "\n",
      "[1255 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to divide by the first row in the group\n",
    "def index_to_base(x):\n",
    "    return x / x.iloc[0]\n",
    "\n",
    "# Use transform() to apply the indexing function to each group\n",
    "faang_indexed = faang_df.groupby('ticker').transform(index_to_base)\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "print(faang_indexed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
